# Statistics {#statistics}

Having described our data and its DGP, we now move on to describing 
statistics calculated using our data.

::: goals
***Chapter goals***

In this chapter we will:

- Model the random process generating a data set.
- Apply and interpret the assumption of simple random sampling, and compare
  it to other sampling schemes.
- Use the theory of probability and random variables to model the statistics
  we have calculated from a data set.
- Calculate and interpret the mean and variance of a statistic from
  its sampling distribution.
:::

## Data and the data generating process

Having invested in all of the probabilistic preliminaries, we can finally talk about
data.  Suppose for the rest of this chapter that we have a data set 
called $D_n$. 

In this chapter, we will assume that $D_n = (x_1,x_2,\ldots,x_n)$ 
is a data set with one variable and $n$ observations.  We use 
$x_i$ to refer to the value of our variable for an arbitrary 
observation $i$.

In real-world analysis, data tends to be more complex:

- In most applications, it will be a simple table of numbers with 
  $n$ observations (rows) and $K$ variables (columns). 
- However, it is occasionally something more abstract.  For example,
  the data set at https://www.kaggle.com/c/dogs-vs-cats is a big
  folder full of dog and cat photos.
  - A great deal of research in the statistical field of machine 
    learning has been focused on developing methods for determining 
    if a particular photo in this data set shows a dog or a cat.  

Although our examples will all be based on simple data sets, many
of our concepts and results can be applied to more complex data.

::: example
**Data from 3 roulette games**

Suppose we have a data set $D_3$ providing the result of $n=3$ independent games of roulette.  Let $b_i$ be the outcome in game $i$, and 
let $x_i$ be the result of a bet on red:
  $$x_i = I(b_i \in RED) = \begin{cases} 1 & b_i \in RED \\ 0 & b_i \notin RED \\ \end{cases}$$
Then $D_n = (x_1,x_2,x_3)$. For example, if red loses the first
two games and wins the third game we have $D_n = (0,0,1)$.
:::

Our data set $D_n$ is a set of $n$ *numbers*, but we can also think of 
it as a set of $n$ *random variables* with unknown joint distribution 
$P_D$. The distinction here is a hard one for students to make, so give 
it some thought before proceeding.

The joint distribution of $D_n$ is called its ***data generating process*** 
or DGP. The exact DGP is assumed to be unknown, but we usually have at 
least some information about it.

::: example
**The DGP for the roulette data**

The joint distribution of $D_n$ can be derived. Let 
  $$p = \Pr(b \in Red)$$
We showed in a previous chapter that $p \approx 0.486$ if the roulette
wheel is fair.  But rather than assuming it is fair, let's treat $p$
as an unknown parameter.

The PDF of $x_i$ is
  $$f_x(a) = \begin{cases}(1-p) & a = 0 \\ p & a = 1 \\ 0 & \textrm{otherwise} \\ \end{cases} $$
Since the random variables in $D_n$ are independent, their
joint PDF is:
  $$\Pr(D_n) = f_x(x_1)f_x(x_2)f_x(x_3) = p^{x_1+x_2+x_3}(1-p)^{3-x_1-x_2-x_3}$$
Note that even with a small data set of a simple random variable,
the joint PDF is not easy to calculate.  Once we get into larger
data sets and more complex random variables, it can get very 
difficult.  That's OK, we don't usually need to calculate it - we
just need to know that it *could* be calculated.
:::

### Simple random sampling

In order to model the data generating process, we need to model the entire 
joint distribution of $D_n$.  As mentioned earlier, this means we must
model both:

- The marginal probability distribution of each $x_i$
- The relationship between the $x_i$'s

Fortunately, we often can simplify this joint distribution quite a bit 
by assuming that $D_n$ 
is ***independent and identically distributed*** (IID) 
or a ***simple random sample*** from a large ***population***.

A simple random sample has two features:

1. **Independent**: Each $x_i$ is independent of the others.
2. **Identically distributed**: Each $x_i$ has the same (unknown)
   marginal distribution.

This implies that its joint PDF can be written:
  $$\Pr(D_n = (a_1,a_2,\ldots,a_n)) = f_x(a_1)f_x(a_2)\ldots f_x(a_n)$$
where $f_x(a) = \Pr(x_i = a)$ is just the marginal PDF of a single 
observation. Independence allows us to write the joint PDF as the 
product of the marginal PDFs for each observation, and identical 
distribution allows us to use the same marginal PDF for each 
observation.

The reason we call this "independent and identically distributed"
is hopefully obvious, but what does it mean to say we have a 
"random sample" from a "population"? Well, one simple way of 
generating an IID sample is to:

1. Define the population of interest, for example all Canadian residents.
2. Use some purely random mechanism[^602] to choose a small subset of cases
  from this population.
   - The subset is called our ***sample***
   - "Purely random" here means some mechanism like a computer's 
     random number generator, which can then be used to 
     dial random telephone numbers or select cases from a list.
3. Collect data from every case in our sample, usually by
   contacting them and asking them questions (survey).

It will turn out that a moderately-sized random sample provides 
surprisingly accurate information on the underlying population.

[^602]: As a technical matter, the assumption of independence requires
  that we sample *with replacement*. This means we allow
  for the possibility that we sample the same case more than once.
  In practice this doesn't matter as long as the sample is small 
  relative to the population.
  
::: example
**Our roulette data is a random sample**

Each observation $x_i$ in our roulette data is an independent
random draw from the $Bernouilli(p)$ distribution where 
$p = \Pr(b \in Red)$.

Therefore, this data set satisfies the criteria for a simple 
random sample.
:::

### Time series data

Our employment data set is an example of ***time series*** data; it is made of
observations of each variable at regularly-spaced points in time.
Most macroeconomic data - GDP, population, inflation, interest rates - are
time series.

Time series have several features that are inconsistent with the random 
sampling assumption:

- They usually have clear *time trends*.  
  - For example, Canada's real GDP has been steadily growing for as 
    long as we have data.
  - This violates "identically distributed" since 2010 GDP is drawn 
    from a distribution with a higher expected value than the
    distribution for 1910 GDP.
- They usually have clear recurring cyclical patterns or *seasonality*.  
  - For example, unemployment in Canada is usually lower from September 
    through December.
  - This also violates "identically distributed" since February 
    unemployment has a higher expected value than November 
    unemployment.
- They usually exhibit what is called *autocorrelation*. 
  - For example, shocks to the economy that affect GDP in one month or
    quarter (think of COVID or a financial crisis) are likely to have 
    a similar (if smaller) effect on GDP in the next month or quarter.  
  - This violates "independence" since nearby time periods are positively
    correlated.

We can calculate statistics for time series, and we already did in Chapter
\@ref(basic-data-analysis-with-excel).  However, time series data 
often requires more advanced techniques than we will learn in this 
class.  ECON 433 addresses time series data.

### Other sampling models

Not all useful data sets come from a simple random sample or a time series. For 
example:

- A ***stratified sample*** is collected by dividing the population into
  *strata* (subgroups) based on some observable characteristics, and then randomly
  sampling a predetermined number of cases within each strata. 
  - Most professional surveys are constructed from stratified samples 
    rather than random samples.
  - Stratified sampling is often combined with *oversampling* of 
    some smaller strata that are of particular interest. 
    - The LFS oversamples residents of Prince Edward Island (PEI) because a
      national random sample would not catch enough PEI residents to
      accurately measure PEI's unemployment rate.
    - Government surveys typically oversample disadvantaged groups.
  - Stratified samples can usually be handled as if they were from
    a random sample, with some adjustments.
- A ***cluster sample*** is gathered by dividing the population
  into ***clusters***, randomly selecting some of these clusters, and
  sampling cases within the cluster.  
  - Educational data sets are often gathered this way: we pick a random 
    sample of schools, and then collect data from each student 
    within those schools.
  - Cluster samples can usually be handled as if they were from
    a random sample, with some adjustments.
- A ***census*** gathers data on every case in the population. 
  - For example, we might have data on all 50 US states, or all 
    10 Canadian provinces, or all of the countries of the world. 
  - Data from administrative sources such as tax records or 
    school records often cover the entire population of interest as well. 
  - Censuses are often treated as random samples from some hypothetical
    population of "possible" cases.
- A ***convenience sample*** is gathered by whatever method is convenient.
  - For example, we might gather a survey from people who walk by,
    or we might recruit our friends to participate in the survey.
  - Convenience samples are the worst-case scenario; in many cases they
    simply aren't usable for accurate statistical analysis.
 
Many data sets combine several of these elements.  For example,
Canada's unemployment rate is calculated using data from the 
Labour Force Survey (LFS). The LFS is built from a stratified 
sample of the civilian non-institutionalized working-age population 
of Canada. There is also some clustering: the LFS will typically
interview whole households, and will do some geographic clustering
to save on travel costs. The LFS is gathered monthly, and the
resulting unemployment rate is a time series.

### Sample selection and representativeness

Random samples and their close relatives have the feature that they
are ***representative*** of the population from which they are 
drawn. In a sense that will be made more clear over the next few chapters,
any sufficiently large random sample "looks just like" the population.

Unfortunately, a simple random is quite difficult to collect from
humans. Even if we are able to randomly select cases, we often
run into the following problems:

- ***Nonresponse*** occurs when a sampled individual does not provide
  the information requested by the survey
  - ***Survey-level*** nonresponse occurs when the sampled individual does not
    answer any questions. 
      - This can occur if the sampled individual cannot be found, refuses 
        to answer, or cannot answer (for example, is incapacitated due to
        illness or disability).
      - Recent response rates to telephone surveys have been around 9\%,
        implying over 90\% of those contacted do not respond.
  - ***Item-level*** nonresponse occurs when the sampled individual does 
    not answer a particular question.  
    - This can occur if the respondent refuses to answer, or the 
      question is not applicable or has no valid answer.
    - Item-level nonresponse is particularly common on sensitive questions
      including income.
- ***Censoring*** occurs when a particular quantity of interest cannot be
  observed for a particular case.  Censored outcomes are extremely common
  in economics, for example:
    - In labour market analysis, we cannot observe the market wage for
      individuals who are not currently employed. 
    - In supply/demand analysis, we only observe quantity supplied and
      quantity demanded at the current market price.
      
There are two basic solutions to these problems:

- ***Imputation***: we assume or ***impute*** values for all missing 
  quantities. For example, we might assume that the wage of each 
  non-employed worker is equal to the average wage among employed workers
  with similar characteristics.
- ***Redefinition***: we redefine the population so that our data
  can be correctly interpreted as a random sample from that population.
  For example, instead of having a random sample of *Canadians*, we can
  interpret our data as a random sample of 
  *Canadians who would answer these questions if asked*.
  
  
This is not an issue that has a purely technical solution, but requires
careful thought instead.  If we are imputing values, do we believe that
our imputation method is reasonable?  If we are redefining the population,
is the redefined population one we are interested in?  There is no right
or wrong answers to these questions, and sometimes our data are simply
not good enough to answer our questions.

:::fyi
**Nonresponse bias in recent US elections**

Going into both the 2016 and 2020 US presidential elections, polls
indicated that the Democratic candidate had a substantial lead over
the Republican candidate: 

- Hillary Clinton led Donald Trump by 4-6\% nationally in 2016 
- Joe Biden led Trump by 8\% nationally in 2020.  

The actual vote was much closer:

- Clinton won the popular vote (but lost the election) by 2\%
- Biden won the popular vote (and won the election) by about 4.5\%.  

The generally accepted explanation among pollsters for the clear 
disparity between polls and voting is systematic nonresponse: for
some reason, Trump voters are less likely to respond to
polls. Since most people do not respond to standard telephone polls
any more (response rates are typically around 9\%), it does not
take much difference in response rates to produce a large difference
in responses.  For example, suppose that:

- We call 1,000 voters
- These voters are equally split, with 500 supporting Biden and 500
  supporting trump.
- 10\% of Biden voters respond (50 voters)
- 8\% of Trump voters respond (40 voters)

The overall response rate is 9\% (similar to what we usually see in surveys),
Biden has the support of $50/90 = 56\%$ of the respondents
while Trump has the support of $40/90 = 44\%$.  Actual support is
even, but the polls show a 12 percentage point gap in support, entirely
because of the small difference in response rates.

Polling organizations employ statisticians who are well aware
of this problem, and they made various adjustments after 2016 to address
it. For example, most now weight their analysis by education, 
since more educated people tend to have a higher response rate.
Unfortunately, the 2020 results indicate that this adjustment was 
not enough.  Some pollsters have argued that it makes
more sense to just assume the nonresponse bias is 2-3\% and adjust 
the numbers by that amount directly.
:::

## Statistics and their properties

Suppose we have some ***statistic*** $s_n =s(D_n)$, i.e., a number 
that is calculated from the data.

  - Since the data is observed/known, the value of the statistic
    is observed/known.
  - Since the elements of $D_n$ are random variables, $s_n$ is also 
    a random variable with a well-defined (but unknown) probability
    distribution that depends on the unknown DGP.


::: example
**Roulette wins**

In our roulette example, the total number of wins is:
  $$R = x_1 + x_2 + x_3$$
Since this is a number calculated from our data, it is a statistic.

Since $x_i \sim Bernoulli(p)$, we can show that $R \sim Binomial(3,p)$. 
:::

### Some important statistics

I will use $s_n$ to represent an abstract statistic, but we will often
use other notation to talk about specific statistics.

The most important statistic is the ***sample average*** which is 
defined as: 
  $$\bar{x}_n = \frac{1}{n} \sum_{i=1}^n x_i$$
We will also consider several other commonly-used univariate 
statistics:

- The ***sample variance*** of $x_i$ is defined as: 
  $$s_x^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2$$
  A closely-related statistic is the ***sample standard deviation***
  $s_x = \sqrt{s_x^2}$ which is the square root of the sample variance.
- The ***sample frequency*** or ***relative sample frequency***
  of the event $x_i \in A$ is defined as the proportion of cases
  in which the event occurs:
  $$\hat{f}_A = \frac{1}{n} \sum_{i=1}^n I(x_i \in A)$$
  A closely-related statistic is the ***absolute sample frequency***
  $n\hat{f}_A$ which is the *number* of cases in which the event 
  occurs.
- The ***sample median*** of $x_i$ is defined as:
  $$\hat{m}_x = m: \begin{cases} \hat{f}_{x < m} \leq 0.5 \\ \hat{f}_{x > m} \leq 0.5 \\ \end{cases}$$

### The sampling distribution

Since the data itself is a collection of random variables, any statistic
calculated from that data is also a random variable, with a probability
distribution that can be derived from the DGP. 

::: example
**The sampling distribution of the sample frequency**

Calculating the exact probability distribution of most statistics is 
quite difficult, but it is easy to do for the sample frequency. Let 
$p =\Pr(x_i \in A)$.
Then:
  $$n\hat{f}_A \sim Binomial(n,p)$$
In other words, we can calculate the exact probability distribution of
the sample frequency using the formula for the binomial distribution.
:::

Unfortunately, most statistics typically have sampling distributions that
are quite difficult to calculate.  

::: fyi
To see why the sampling distribution of a statistic is so difficult
to calculate, suppose we have a discrete random variable $x_i$ whose 
support $S_x$ has five elements. Then we need to calculate the sampling
distribution of our statistic by adding its probability up across
the support of $D_n$.  The support has $5^n$ elements, a number that 
can quickly get very large.  

For example, a typical data set in 
microeconomics has at least a few hundred or a few thousand observations.
With 100 observations, $D_n$ can take on $5^{100} \approx 7.9 \times 10^{69}$
(that's 79 followed by 68 zeros!) distinct values.  With 
1,000 observations , $D_n$ can take on $5^{1000}$ distinct values,
a number too big for Excel to even calculate.
:::

### The mean and variance

If our statistic has a probability distribution, it (usually) has a mean
and variance as well. Under some circumstances, we can calculate them.

::: example
**The mean of the sample average**

Let $\mu_x = E(x_i)$ be the mean of $x_i$  Then the mean of $\bar{x}$ is:
$$E(\bar{x}_n) = E\left( \frac{1}{n} \sum_{i=1}^n x_i\right) = \frac{1}{n} \sum_{i=1}^n E\left( x_i\right) = \frac{1}{n} \sum_{i=1}^n \mu_x = \mu_x$$
:::

This is an important and general result in statistics. The mean of the 
sample average in a random sample is identical to the mean of the random 
variable being averaged.
  $$E(\bar{x}_n) = E(x_i)$$
We have shown this property specifically for a random sample, but it 
holds under many other sampling processes.

The variance of the sample average is not equal to the variance 
of the random variable being averaged, but they are closely related.

::: example
**The variance of the sample average**

To keep the math simple, suppose we only have $n = 2$ observations.  Then
the sample average is:
  $$\bar{x} = \frac{1}{2}(x_1 + x_2)$$
By our earlier formula for the variance:
  \begin{align}
    var(\bar{x}) &= var\left(\frac{1}{2}(x_1 + x_2)\right) \\
      &= \left(\frac{1}{2}\right)^2 var(x_1 + x_2) \\
      &= \frac{1}{4} \left( \underbrace{var(x_1)}_{\sigma_x^2} + 
          2 \underbrace{cov(x_1,x_2)}_{0 \textrm{(independence)}} + \underbrace{var(x_2)}_{\sigma_x^2} \right) \\
      &= \frac{1}{4} \left( 2 \sigma_x^2 \right) \\
      &= \frac{\sigma_x^2}{2} \\
  \end{align}
:::

More generally, the variance of the sample average in a random sample
of size $n$ is:
  $$var(\bar{x}_n) = \frac{\sigma_x^2}{n}$$
where $\sigma_x^2 = var(x_i)$.

Other commonly-used statistics also have a mean and variance.

::: example
**The mean and variance of the sample frequency**

Since the absolute sample frequency has the binomial distribution, 
we have already seen its mean and variance.  Let $p = \Pr(x_i \in A)$.
Then $n\hat{f}_A \sim Binomial(n,p)$ and:
  $$E(n\hat{f}_A) = np$$
  $$var(n\hat{f}_A) = np(1-p)$$

Applying the usual rules for expected values, the mean and variance of
the relative sample frequency is:
  $$E(\hat{f}_A) = \frac{E(n\hat{f}_A)}{n} = \frac{np}{n} = p$$ 
  $$var(\hat{f}_A) = \frac{var(n\hat{f}_A)}{n^2} = \frac{np(1-p)}{n^2} = \frac{p(1-p)}{n} $$
:::

