# Probability and random events

```{r setup3, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(VennDiagram)
library(RColorBrewer)
```

***Probability*** is a method of mathematically modeling a random process
so that we can understand it and/or make predictions about its future 
results. Probability is an essential tool for casinos, as well as for
banks, insurance companies, and any other businesses that manage risks.

***Statistics*** is a method of using data from a random process
to make inferences about that process. Again, the purpose is to understand
the process, and maybe to make predictions for the future.  We will
talk about statistics in later chapters.

::: goals
**Chapter goals**

In this chapter we will learn how to:

 - Model random events using the tools of probability
 - Calculate and interpret marginal, joint, and conditional probabilities
 - Interpret and use the assumptions of independence and equal outcome probability

We will also review some math needed to learn these skills.

   - Sets
   - Functions
   - Sequences, limits and summations
:::

## Mathematical background

I'll start by reviewing some math we will use. All of this math is covered
in an introductory calculus class so most of you have seen it before.
Those of you who are currently taking introductory calculus will see
it in that class soon enough.

But you may have forgotten it, or never understood it in the first place.
So I will go briefly through the most important terminology and ideas.
If there's anything you don't understand please ask me to clarify. 

### Sets

The most fundamental notion in mathematics is the idea of a ***set***. 
A set is typically described as a collection or gathering
of distinct objects. These objects are called the ***elements*** 
of the set.

#### Defining a set

We have several ways of defining or describing sets. The simplest is ***enumeration***, 
which means you just list the elements:
$$A = \{1,2,3\}$$
$$B = \{Avocado, Banana\}$$

Notice that the list of elements is surrounded by curly brackets.

A second way of defining a set is to use ***set-builder notation***.
Set builder notation defines sets in terms of rules they must satisfy.
For example:

$$C = \{x \in B: x \textrm{ is yellow} \}$$

We read this as "$C$ is the set of all $x$ in the set $B$ such that $x$ is yellow." In other words:

 - Avocados are not in set $C$ because they are not yellow. 
 - Bananas are in set $C$ because they are in set $B$ and they are yellow. 
 - Lemons are not in set $C$ because they are not in set $B$.

We sometimes leave the initial set implicit:
   $$C = \{x: x \textrm{ is yellow} \}$$
We would interpret this as saying that $C$ is the set of *everything* that is yellow.

There are a few special sets defined by convention:

 - The ***empty set***, usually written $\emptyset$,
   is a set with no elements.
 - The ***universal set*** (usually written $\mathbb{U}$) 
   is the set of "everything" we might be talking about in
   a given context. The universal set is usually implicit, 
   though we will occasionally need to define it explicitly.
 - The set of ***integers*** is usually written as $\mathbb{Z}$.
 - The set of ***rational*** numbers is usually written as $\mathbb{Q}$.
 - The set of ***real numbers*** is usually written as $\mathbb{R}$.
 - The set of positive integers is $\mathbb{Z}_+$, the set of 
   positive real numbers is $\mathbb{R}_+$, etc.


#### Characteristics of a set

The size or ***cardinality*** of the set $A$, usually written $|A|$, 
is simply the number of elements it has. For example:

$$|\{1,2,3\}| = 3$$
$$|\{Avocado,Banana\}| = 2$$
$$|\mathbb{Z}| = \infty$$ 
$$|\emptyset| = 0$$

- $A$ is a ***singleton*** if $|A| = 1$.
- $A$ is a ***finite set*** if $|A|$ is a finite number. 

#### Set algebra
 
Let $A$ and $B$ be two sets. We have several ways of describing
how they are related:

 - $A$ and $B$ are ***identical*** (written $A = B$) if they contain the same elements. For example:
   $$\{1,2\} = \{1,2\}$$
   $$\{1,2\} = \{2,1\}$$
   $$\{1,2\} \neq \{1,2,3\}$$
   $$\{1,2\} \neq \{1,3\}$$
 - $A$ and $B$ are ***disjoint*** if they have no elements in common. 
   For example:
   - $\{1,2\}$ and $\{3,4\}$ are disjoint.
   - $\{1,2,3\}$ and $\{3,4\}$ are not disjoint.
 - $A$ is a ***subset*** of $B$ (written $A \subseteq B$ or $A \subset B$ ) if all elements 
   of $A$ are also elements of $B$. 
   For example:
   $$\{1,2\} \subset \{1,2\}$$
   $$\{1,2\} \subset \{1,2,3\}$$
   $$\{1,2\} \not\subset \{1,3\}$$

We can also perform various mathematical operations on sets.

- The ***intersection*** of $A$ and $B$ (usually written $A \cap B$) 
   is the set of everything that is an element of both $A$ *and* $B$.
   $$A \cap B = \{x: x \in A \textrm{ and } x \in B\}$$
   For example:
   $$\{1,2\} \cap \{3,4\} = \emptyset$$
   $$\{1,2,3\} \cap \{3,4\} = \{3\}$$
- The ***union*** of $A$ and $B$ (usually written $A \cup B$)
   is the set of everything that is in $A$ *or* $B$ (or both):
      $$A \cup B = \{x: x \in A \textrm{ or } x \in B\}$$
   For example:
   $$\{1,2\} \cup \{3,4\} = \{1,2,3,4\}$$
   $$\{1,2,3\} \cup \{3,4\} = \{1,2,3,4\}$$
- The ***complement*** of $A$ ***in*** $B$, also called the ***set difference***
   is written as $B - A$ or $B \setminus A$ and defined as the set of everything 
   in $B$ that is *not* in $A$:
   $$ B - A = \{x \in B: x \notin A\}$$

   We will also refer to the ***complement*** of $A$, which is written
   $A'$, $\neg A$ or $A^c$, and is simply everything that is not in $A$:
   $$ A^c = \mathbb{U} - A = \{x: x \notin A\}$$

The combination of these basic definitions, relationships (identity, 
element, subset) and operations (union, intersection, complement) 
are collectively called ***set algebra***.

::: fyi
**Some standard results about sets**

Given the basic components of set algebra, we can establish many useful rules. 
This is not a course on set theory, so I will simply list some of the most important rules
for your reference. None of these rules is difficult to prove, and most of them should 
make intuitive sense to you.

   - Non-negative cardinality: 
      $$|A| \geq 0$$
   - Cardinality of unions: 
      $$|A \cup B| = |A| + |B| -|A \cap B| \leq |A| + |B|$$
   - Cardinality of intersections: 
      $$|A \cap B| \leq min(|A|,|B|)$$
   - Commutative laws:
      $$A \cup B = B \cup A$$
      $$A \cap B = B \cap A$$
   - Associative laws:
      $$(A \cup B) \cup C = A \cup (B \cup C)$$
      $$(A \cap B) \cap C = A \cap (B \cap C)$$
   - Distributive laws:
      $$A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$$
      $$A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$$
   - Identity laws:
      $$A \cup \emptyset = A$$
      $$A \cap \mathbb{U} = A$$
   - Complement laws:
      $$A \cup A^c = \mathbb{U}$$
      $$A \cap A^c = \emptyset$$
      $$\mathbb{U}^c = \emptyset$$
      $$\emptyset^c = \mathbb{U}$$
   - Double-complement law:
      $$(A^c)^c = A$$
   - Idempotent laws:
      $$A \cup A = A$$
      $$A \cap A = A$$
   - Domination laws:
      $$A \cup \mathbb{U} = \mathbb{U}$$
      $$A \cap \emptyset = \emptyset$$
   - DeMorgan's laws:
      $$(A \cup B)^c = A^c \cap B^c$$
      $$(A \cap B)^c = A^c \cup B^c$$

Notice that these rules generally come in pairs, with union and intersection switched. This
duality is a general feature of set algebra.
:::

### Functions

#### Definition of a function

A ***function*** is a rule that matches ("maps") elements of one set (called the ***domain***
of the function) to elements of another set (called the ***range*** of the function).

We will use the notation $f: D \rightarrow R$ to declare that a particular function $f$ has 
domain $D$ and range $R$. For example:

   - $f:\mathbb{R} \rightarrow \mathbb{R}$ means that the function $f$ takes a
      real number, and returns a real number.
   - $g:\mathbb{Z}_+ \rightarrow \{0,1\}$ means that the function $g$ takes a
      positive integer, and returns either zero or one.
   - $s: \{Avocado,Banana,Cantaloupe\} \rightarrow \mathbb{Z}$ means that the function $s$
      takes either $Avocado$, $Banana$ or $Cantaloupe$ and returns an integer.

If a function has a finite domain, we can define it by enumeration. For example:
   $$s(Avocado) = 1$$
   $$s(Banana) = 0$$
   $$s(Cantaloupe) = 500$$
We can also define a function by mathematical expression:
   $$f(x) = x^2$$
   $$g(y) = \ln(y)$$
Finally we can just refer to a function without defining exactly what it is,
just like we can talk about a variable $x$ or a set $A$ without assigning
it a particular value.

::: fyi
**Function or multiplication?**

Students sometimes confuse functions and multiplication because of the way we 
conventionally write functions, students sometimes get confused. Suppose I write this:
   $$z = f(x+y)$$
There are two possible interpretations of this statement:

   1. $f$ is a number, so $z$ is equal to the number $f$ times the number $(x+y)$.
   2. $f$ is a function, so $z$ is equal to the function $f$ applied to the 
      number $(x+y)$.

It is sometimes clear from the context which  of these interpretations is correct 
in a given problem. But please ask if you aren't sure.
:::

#### The indicator function

The ***indicator function*** is a special kind of function whose argument
is a *statement*. The indicator function returns a value of 1
if the statement is true, and 0 if it is false. For example:

$$I(3 < 5) = 1$$
$$I(3 = 5) = 0$$
$$I(\textrm{Ottawa is the capital of Canada}) = 1$$
$$I(\textrm{Ottawa is in Alberta}) = 0$$
We use indicator functions all the time in statistics because they allow us
to convert a qualitative statement such as "Bob is employed" 
into a quantitative statement: $I(\textrm{Bob is employed}) = 1$.

### Sequences, limits and summations

#### Cartesian products

The ***Cartesian product*** of two sets $A$ and $B$, usually written 
$A \times B$, is the set of all ordered pairs of elements in the two sets.
For example, if $A = \{1,2\}$ and $B = \{3,4\}$ then
$$ A \times B = \{(1,3),(1,4),(2,3),(3,4)\}$$
Another example of a Cartesian product is:
   $$\mathbb{R}^2 = \mathbb{R} \times \mathbb{R}$$
the set of ordered pairs of real numbers.  For example $(0,3)$ and $(0.427,2000)$ 
are both elements of $\mathbb{R}^2$.

We will also be interested in the set of ordered ***sequences*** of $n$ real 
numbers (where $n$ is some positive integer):
   $$\mathbb{R}^n = \mathbb{R} \times \mathbb{R} \times \cdots \times \mathbb{R}$$
For example the ordered sequence $(1,3,5,7)$ is an element of the set $\mathbb{R}^4$.

#### Sequences 

A sequence is much like a set but with two important differences:

1. Order matters. For example:  
   - $\{1,2\} = \{2,1\}$ 
   - $(1,2) \neq (2,1)$.
2. Elements can be repeated.  For example 
   - $\{1,3,1\}$ is not a valid set.
   - $(1,3,1)$ is a valid sequence 

Like sets, sequences can be empty, finite, or infinite. 

- $()$ is an empty sequence.
- $(2,4,6)$ is a finite sequence.
- $(2,4,6,\ldots)$ is an infinite sequence.

As you can see from the examples above, we usually distinguish between sequences
and sets by using parentheses for sequences and curly brackets for 
sets.

We usually number each element in a sequence. The number assigned to a 
given element is called its ***index***. Typically, the first 
element in a sequence is numbered either 0 or 1, and the 
remaining elements are numbered sequentially after that.

We can define sequences by simple enumeration as in the examples
above. These are a few other ways of defining a sequence:

 - As variables with subscripts: $(x_1,x_2,x_3)$. We will usually 
   use a variable as the subscript $x_i$ when we want to talk
   about an arbitrary element of the sequence.
 - As functions of other sequences: $y_i = \ln(x_i)$.
 - As functions of the index itself $x_i = a^i$

#### Limits

Let $(x_1,x_2,\ldots)$ be a sequence of infinite length.
We say that $x^*$ is the ***limit*** of this 
sequence if $x_i$ gets closer and closer to $x^*$ as
$i$ gets bigger and bigger.

- The limit of a sequence can be a number.
- The limit of a sequence can be $\infty$ or $-\infty$.
- Not all sequences have limits.

For example:
$$ \lim (1,1,1,\ldots) = 1$$
$$\lim\left(1,\frac{1}{2},\frac{1}{3},\ldots\right) = 0$$
$$\lim(1,2,3,\ldots) = \infty$$
$$\lim(-1,-2,-3,\ldots) = -\infty$$
$$\lim(0,1,0,1,0,1,\ldots) \textrm{ does not exist}$$

You will learn or have learned the formal definition of a limit in your 
calculus course.  I won't make you re-learn it for this class, but here it is: 

*Definition of a limit*: Let $(x_1,x_2,\ldots)$ be a sequence of infinite length.  We say that 
the number $c$ is the limit of this sequence: 
   $$\lim_{i\rightarrow \infty} x_i = c$$
if for any $\delta > 0$ there exists an $N_{\delta}$ such that
   $$|x_i - c| < \delta \textrm{ for all } i > N_{\delta}$$

#### Summations

Many statistics we are calculating are constructed by adding 
up a sequence of numbers. It will be convenient to 
use the ***summation operator***, which looks like this:

$$\sum_{i=1}^3 x_i = x_1 + x_2 + x_3$$ 
$$\sum_{i=1}^n x_i = x_1 + x_2 + \cdots + x_n$$
$$\sum_{i=1}^{\infty} x_i = x_1 + x_2 + \cdots$$
$$\sum_{i \in \{1,2,3\}} x_i = x_1 + x_2 + x_3$$ 
Notice that an expression using the summation operator has several components:

   - The summation sign $\sum$.
   - An expression identifying the index variable, and what values it takes on. The 
      index variable is usually but not always called $i$.
      - $\sum_{i \in \{1,2,3\}}$ means we add up over all of the values  
         in the set $\{1,2,3\}$.
      - $\sum_{i = 1}^{n}$ means we add up over all of the integers between 1
         and $n$.
   - An expression identifying what is to be added up. It is usually but not always
      a function of the index variable. For example:
         $$\sum_{i=1}^3 \ln(x_i) = \ln(x_1) + \ln(x_2) + \ln(x_3)$$
         $$\sum_{j=1}^3 \beta^j = \beta + \beta^2 + \beta^3$$
         $$\sum_{i=1}^3 3 = 3 + 3 + 3 = 9$$
         $$\sum_{i=1}^3 2x = 2x + 2x + 2x = 6x$$

We can also use multiple summation operators in an expression. For example:
$$\sum_{j=1}^2 \sum_{i=1}^2 x_iy_j = \sum_{j=1}^2 (x_1y_j + x_2y_j) = x_1y_1 + x_1y_2 + x_2y_1 + x_2y_2$$ 

The summation operator looks fancy, but remember it is just a concise way
of describing a sum. You learned the basic properties of addition 
and multiplication in Grade 3:

- Associative property: 
   $$(a + b) + c = a + (b+c)$$
   $$(ab)c = a(bc)$$
- Commutative property: 
   $$a + b = b + a$$
   $$ab = ba$$
- Distributive property: 
   $$a(b+c) = ab + ac$$

Summation operators also obey these properties:

- The associative and commutative properties allow you to switch any two summation
   operators:
  $$\sum_{i \in A} \sum_{j \in B} x_iy_j = \sum_{j \in B} \sum_{i \in A}  x_iy_j$$
- The distributive property allows you to take any constant out of the summation 
   operator
  $$\sum_{i=1}^n ax_i = a \sum_{i=1}^n x_i $$

We will use these two results later on.

## Random events and probability

Having reviewed the math, we now go on to develop the basic concepts in probability.

::: example
**Example application: Roulette**

We will develop ideas by considering the casino game of **Roulette**.
The picture below shows what a roulette wheel looks like.

![roulette wheel image](roulette.svg)
Source: <a href="https://www.vecteezy.com/free-vector/roulette">Roulette Vectors by Vecteezy</a>

Here are the rules:

- It features 
  - a ball.
  - a spinning wheel with numbered/colored slots. 
  - a table on which to place bets
- The slots are numbered from 0 to 36
  - Slot number 0 is green
  - 18 slots are red
  - 18 slots are black.
  - The picture above depicts an American roulette table, which has an 
    additional green slot labeled "00",
  - I will assume we have a European roulette table, which doe snot include
    the "00" slot.
- Players can place various bets on the table including:
  - Red (ball lands in a red slot) pays \$1 per \$1 bet
  - Black (ball lands in a black slot) pays \$1 per \$1 bet
  - A straight bet on any specific number (ball lands on that number)
    pays \$35 per \$1 bet

Like other casino games, a roulette game is an example of a random process.
Something will happen, it matters (to the players and the casino) what
will happen, but we don't know in advance what will happen.
:::


### Outcomes and events

To build a probabilistic model of a random process, we start by defining
the ***outcome*** we are interested in.  An outcome can be a simple yes/no
result, it can be a number, or it can be a much more complex object.

::: example
In our roulette example, the outcome can be defined as the number of 
the slot in which the ball lands.  Call that number $b$.
:::

The set of all possible outcomes is called the ***sample space***

::: example
In our roulette example, the sample space can be defined as
the set of all numbers the ball can land on:
   $$\Omega = \{0,1,2,\ldots,36\}$$
:::

Next, we define a set of ***events*** that we are interested in.
We can think of an event as either:

 - A statement that is either true or false OR
 - A subset of the sample space 

These two concepts are equivalent, though the subset concept 
makes the math clearer.
 
::: example

These roulette events are well-defined for our sample space: 

- Ball lands on 15:
  $$b \in \{15\}$$
- Ball lands on red:
$$b \in RED = \{1,3,5,7,9,12,14,16,18,19,21,23,25,27,30,32,34,36\}$$
- Ball lands on black:
$$b \in BLACK = \{2,4,6,8,10,11,13,15,17,20,22,24,26,28,29,31,33,35\}$$
- Ball lands on one of the first 12 numbers:
$$b \in FIRST12 = \{1,2,3,4,5,6,7,8,9,10,11,12\}$$
:::

Since events are sets, we can use the terminology and mathematical
tools for sets. 

::: example
In our roulette example:

- Events are *identical* if they contain the same outcomes:
  - The event "ball lands on 12 " and "a bet on 12 wins"
    are identical since $\{12\} = \{12\}$.
- An event *implies* another event if all of its outcomes are also in 
  the implied event 
  - The event "ball lands on 12"  implies the event "ball lands on red" 
    since $\{12\} \subset RED$.
- Events are *disjoint* if they share no outcomes:
  - The events "ball lands on red" and "ball lands on black" are disjoint 
    since $RED \cap BLACK = \emptyset$.
- Any two non-identical outcomes are disjoint
   - The events "ball lands on 12" and "ball lands on 15" are disjoint
     since $\{12\} \cap \{15\} = \emptyset$.
:::

### Probabilities

Our final step is to define a ***probability distribution***
for this random process, which is a function that assigns
a number to each possible event.  The number is called 
the event's ***probability***. 

Probabilties are normally between zero and one:

- If an event has probability zero, it definitely *will
  not* happen
- If an event has probability strictly between zero and
  one, it *might* happen.
- If an event has probability one, it definitely *will* 
  happen.

All valid probability distributions must obey the 
following three conditions:

1. Probabilities are never negative:
   $$\Pr(A) \geq 0$$
2. One of the outcomes will definitely happen:
   $$\Pr(\Omega) = 1$$
3. For any two *disjoint* events $A$ and $B$, the probability that
   $A$ or $B$ happen is the sum of their individual probabilities:
   $$\Pr(A \cup B) = \Pr(A) + \Pr(B)$$

Probability distributions have many other properties, but they
can all be derived from these three.

::: example
Let's assume that the roulette wheel is "fair" in the sense that each 
outcome has the same probability. Now, I should emphasize that this 
doesn't have to be the case, it's just an assumption.  But it's a 
reasonable one in this case because casinos are required
by law to run fair roulette wheels and would be subject to heavy 
penalties if they run unfair wheels. Later on, we will use statistics
to confirm that a roulette wheel is fair.

Call that probability $p$:
   $$p = \Pr(b = 0) = \Pr(b = 1) = \cdots = \Pr(b = 36)$$
To find the value of $p$ we use the rules of probablity. 
By rule #2 of probability, one of the outcomes will happen:
   $$\Pr(\Omega) = 1$$
Since the different outcomes are disjoint, rule #3 implies that:
   $$\underbrace{\Pr(\Omega))}_{1} = \underbrace{\Pr(\{0\})}_{p} + \underbrace{\Pr(\{1\})}_{p} + \cdots + \underbrace{\Pr(\{36\})}_{p}$$
Summarizing this equation:
   $$1 = 37p$$
Solving for $p$ we get:
   $$p = 1/37 \approx 0.027$$
That is, each of the 37 outcomes have a probability of $1/37$.
:::

Since this is an introductory course, our sample space will usually 
contain a finite number of outcomes, as in our roulette example.  In
that case, probability calculations are pretty simple:

- Find the probability of each outcome.
- To find the probability of a specific event, just add up the probabilities
  of its outcomes.
  

::: example
In the roulette example, the probability of any event $A$ is
just the number of outcomes in the event $|A|$ times the 
probability of each outcome $1/37$:
   $$ \Pr(A) = |A|*1/37$$
For example:
$$\Pr(b=12) = |\{12\}|*1/37 = 1/37 \approx 0.027$$
$$\Pr(RED) = |RED|*1/37 = 18/37 \approx 0.486$$
$$\Pr(EVEN) = |EVEN|*1/37 = 18/37 \approx 0.486$$
$$\Pr(FIRST12) = |FIRST12|*1/37 = 12/37 \approx 0.324$$
:::

However, not all sample spaces contain a finite number of outcomes.  For
example, suppose we are interested in using probability to model the
unemployment rate, or a person's income.  Those are real numbers, and can 
take on any of an infinite number of values.  

::: fyi
What does it really mean to say that the probability of the ball landing in 
a red slot is about 0.486? That's actually a tough question. There
are two standard interpretations for probabilities:

- Frequentist or classical interpretation: we are thinking of 
  the random process as something that could be repeated many times,
  and the probability of an event is the approximate fraction of times
  that the event will occur.  That is, if you go to a casino and
  bet 1000 times on Red, you will win about 486 times. 
- Subjectivist or Bayesian interpretation: the random process
  is a one-time occurence, but we have limited information about
  it and the probability of event represents the strength of 
  our belief that the event will happen.

The frequentist interpretation of probability is well-suited
for simple repeated settings like casino games or car insurance,
while the Bayesian interpretation makes more sense for things
like predicting election results.
:::


### Related events

Sometimes are interested in more than one event, and want to talk about how they are 
related. For example:

 - In some casino games like poker or blackjack, players take an additional 
   action after partial information about the outcome is revealed.
 - In politics we often want to predict the winner of an election based on 
   some polls.
 - In economics we often have data on current economic conditions and want 
   to predict future economic conditions.

This section will develop some tools for dealing with the relationship
between different random events.

#### Some standard results about probabilities

Let $A$ and $B$ be two events.  Then:

- Probabilities cannot be higher than one.
   $$\Pr(A) \leq 1$$
- Probabilities of identical events are identical:
   $$A = B \implies \Pr(A) = \Pr(B)$$
- Probabilities of implied events are larger:
   $$A \subset B \implies \Pr(A) \leq \Pr(B)$$
- The probability of an event *not* happening is:
   $$\Pr(A^C) = 1 - \Pr(A)$$
- The probability of nothing happening is:
   $$\Pr(\emptyset) = 0$$
- The probability of either $A$ or $B$ happening is:
  $$\Pr(A \cup B) = \Pr(A) + \Pr(B) - \Pr(A \cap B) \leq \Pr(A) +\Pr(B)$$

These results all follow from the basic rules of probability. They
are not hard to prove, but I will not go through the proofs.

#### Joint probabilities

The ***joint probability*** of two events $A$ *and* $B$ is the probability that
they *both* happen:
  $$\Pr(A \cap B)$$

::: example
For example, consider two roulette events:
  $$Red = \{1,3,5,7,9,12,14,16,18,19,21,23,25,27,30,32,34,36\}$$
  $$Even = \{2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36\}$$
Suppose you are interested in the probability that the ball lands on a number that is both 
red and even. 
  $$\Pr(Red \cap Even) = \Pr(\{12,14,16,18,30,32,34,36\}) = 8/37 \approx 0.216$$
:::

Joint probabilities are just probabilities, so they obey all of the rules of probability.  For example, all joint probabilities are between zero and one.

#### Conditional probabilities

The ***conditional probability*** of an event $A$ ***given*** another
event $B$ is defined as:
   $$\Pr(A|B) = \frac{\Pr(A \cap B)}{\Pr(B)}$$
The conditional probability answers the question: if we already know that 
$B$ is true, what are the chances that $A$ is true?

Conditional probabilities are very important when playing poker. At the beginning
of the game, every player has equal chance of having a winning hand.
But that is no longer true after you see your cards - having "good" cards increases
your chance of winning, and having "bad" cards decreases that chance. In other
words, your bet should be based on $\Pr(win|cards)$ rather than $\Pr(win)$. Good
poker players have detailed knowledge of these conditional probabilities.

::: example
In our roulette example:
   $$\Pr(Red|Even) = \frac{\Pr(Red \cap Even)}{\Pr(Even)} = \frac{8/37}{18/37} \approx 0.444$$
   $$\Pr(b = 26|Even) = \frac{\Pr(b = 26 \cap Even)}{\Pr(Even)} = \frac{1/37}{18/37} \approx 0.056$$
:::

Like joint probabilities, conditional probabilities are just probabilities,
so they obey all of the rules of probability.

#### Independent events 

One common way of modeling joint and conditional probabilities is to assume
that certain events are unrelated to each other.

We say that two events $A$ and $B$ are ***independent*** if their joint
probability is just the two individual probabilities multiplied together:
  $$\Pr(A \cap B) = \Pr(A)\Pr(B)$$
We usually express independence with the notation $A \bot B$.

This definition is not very intuitive, but we can improve upon it by 
doing a little math.  Consider two independent events $A$ and $B$ that
have nonzero probability.  Then by the definition of independence:
  $$\Pr(A|B) = \frac{\Pr(A \cap B)}{\Pr(B)} = \frac{\Pr(A)\Pr(B)}{\Pr(B)} = \Pr(A)$$
By the same reasoning:
  $$\Pr(B|A) = \Pr(B)$$
In other words, knowing that one of these events are true 
tells you nothing useful about whether the other the other event
is true.

Most events we are interested in are not independent because 
they are physically or logically related. 

::: example
Consider the roulette events "Red wins" and "even wins".  We earlier
showed that the unconditional probability of red wins is:
  $$\Pr(Red) = 18/37 \approx 0.486$$
The conditional probability of red given even is:
   $$\Pr(Red|Even) = 8/18 \approx 0.444$$
Since $0.44 \neq 0.486$, these two events are *not* independent.
:::

When would it be reasonable to assume events are independent? 
The typical scenario would be where there is simply no 
physical or logical relationship between them, usually due
to a separation in time and space.

::: example
Suppose that I bring \$100 to a casino this afternoon for a few games 
of roulette. I bet all of my money on red for the first game.

- If I lose, I am broke and stop playing.
- If I win, I keep all of my money (both my intial bet and my winnings)
  on red for the next spin.  
- I keep playing until I run out of money.

After 3 games:

 - If red wins all 3 games, I have $w = \$800$.
 - Otherwise, I have nothing $w = \$0$

What is the probability of each of these events?  Since we can 
assume that each game's outcome is independent, this is an easy
problem:

  $$\Pr(w = \$800) = \Pr(Red_1 \cap Red_2 \cap Red_3) = \Pr(Red_1)\Pr(Red_2)\Pr(Red_3) = (18/37)^3 \approx 0.115$$
   $$\Pr(w = \$0) = 1 - \Pr(w = \$800) \approx 0.885$$
:::

#### Some standard results about joint and conditional probability

In addition to the results we have already discussed, there are two
important results on conditional probabilities:

The first is ***Bayes' law***, which is a rule for determining 
conditional probabilities:
  $$\Pr(A|B) = \frac{\Pr(B|A)\Pr(A)}{\Pr(B)}$$
The second is the ***Law of total probability*** which
is a rule for determining unconditional probabilities 
from conditional probabilities:
  $$\Pr(A) = \Pr(A|B)\Pr(B) + \Pr(A|B^c)\Pr(B^c)$$
These rules follow from our existing results, combined with the
definition of conditional probabilities.  They are easy to
prove, but I won't prove them here.  Instead, I will use
an example to show how they can be useful.

::: example
**False positives in medical testing**

When someone is tested for a disease, the test comes back either
"positive" (the person has the disease) or "negative" (the person does
not have the disease). However, no test is perfect. Sometimes people
who do not have the disease test positive ("false positives") and 
sometimes people who do have the disease test negative 
("false negative").

Let the event $T$ mean a particular patient tests positive
for a disease, and let the event $D$ mean that this patient 
actually has the disease. 

The *sensitivity* of the test is an infected patient's probability
of testing positive:
   $$\Pr(T|D) = p$$
the *specitivity* of the test is a healthy patient's probability
of testing negative:
   $$\Pr(T^c|D^c) = q$$
and the *prevalence* of the infection is the probability that
a given patient has the disease:
   $$\Pr(D) = d$$
Suppose that a patient has tested positive.  What is the
probability that he has the disease, i.e. what the value of
$\Pr(D|T)$?

This is a classic probability question, as it makes use
of Bayes' law and the law of total probability, and it has
important practical usage.  We start by applying
Bayes' rule:
   $$\Pr(D|T) = = \frac{\Pr(T|D)\Pr(D)}{\Pr(T)}$$
We know that $\Pr(T|D)=p$ and $\Pr(D)=r$, so all we need is to find $\Pr(T)$.
Using the law of total probability:
   $$\Pr(T) = \underbrace{\Pr(T|D)}_{p}
              \underbrace{\Pr(D)}_{d} 
            + \underbrace{\Pr(T|D^c)}_{1-q}
              \underbrace{\Pr(D^c)}_{1-d}$$
Plugging these results into our formula:
   $$\Pr(D|T) = = \frac{pd}{pd + (1-q)(1-d)}$$
   
Now, let's try out some numbers:

- Suppose that false positives are rare ($q = 0.99$), false negatives never happen
  ($p=1$) and the disease itself is fairly common ($d = 0.10$).  Then:
   $$\Pr(D|T) = = \frac{1*0.1}{1*0.1 + (1-0.99)*(1-0.1)} \approx 0.917$$
- Suppose that false positives are rare and false negatives never happen, but the 
  disease itself is also rare ($d = 0.001$).  Then
   $$\Pr(D|T) = = \frac{1*0.001}{1*0.001 + (1-0.99)*(1-0.001)} \approx 0.091$$
In other words, the same test has a very different meaning depending
on the prevalence in the population: when the disease is common a positive
test means a 91.7% chance of having the disease, and when the disease
is rare a positive test result means a 9.1% chance of having the disease.
:::
