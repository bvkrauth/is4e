---
title: "Chapter 5: Statistics"
author: "ECON 233, Brian Krauth"
date: "Spring 2021"
output: 
  html_document:
    theme: paper
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter overview

In this chapter we will:

 - Follow good data management practices 
 - Use formulas in Excel
 - Gather resources and tools, including all needed computer software.

# Data and the data generating process

The basic idea of statistical analysis is that we will 

- Treat our data as the result of a random process
  called the ***data generating process***
- Write down a model of the data generating process
  using the tools of probability. 
  - Both the data itself and any statistics 
    calculated from that data can be considered 
    random variables derived from the DGP.
- Our model of the DGP will often include one
  or more unknown features called *parameters*.
- Construct statistics to estimate
  
  
## Random samples from a population

Let $D = (x_1,x_2,\ldots,x_n)$ be a data set with
one variable and $n$ observations.  We can use
$x_i$ to refer to an arbitrary observation $i$.

In order to model the data generating process, we need to
model the entire joint distribution of $D$.  This
means:

- The probability distribution of each $x_i$
- The relationship between the $x_i$'s

Fortunately, we often can simplify this joint distribution
quite a bit by assuming that $D$ 
is ***independent and identically distributed*** (IID) 
or a ***random sample*** from a ***population***.
This means that:

1. Each $x_i$ is independent of the others.
2. Each $x_i$ has the same probability distribution.

The reason we call this "independent and identically distributed"
is hopefully obvious, but what does it mean to say we have a 
"random sample" from a "population"? 

The idea is to suppose we have a very large number of cases 
(say, the entire population of Canada), and we:

1. Use some purely random mechanism to choose a small subset of cases.
   - The subset is called our ***sample***
   - "Purely random" here means some mechanism like a coin flip
     or a computer's random number generator.
2. Collect data from our sample.

This process will produce IID data. It will turn out that a moderately-sized
random/IID sample will usually provide surprisingly accurate information on
the underlying population.

Not all data sets come from a random sample, but many of them do. For 
example, Canada's unemployment rate is calculated using data from the 
Labour Force Survey (LFS). The LFS is a random sample (sort of) of the 
civilian non-institutionalized working-age population of Canada.

## The sampling distribution of a statistic

Let $s = s(D)$ be some statistic calculated from the data set $D$.
To give a few examples
- The ***sample average*** of $x_i$ is defined as: 
  $$\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$$
- The ***sample frequency*** of the event $x_i \in A$ is defined as:
  $$\hat{f}_A = \sum_{i=1}^n I(x \in A)$$
- The ***sample median*** of $x_i$ is defined as:
  $$\hat{m}_x = m: \begin{cases} \hat{f}_{x < m} \leq 0.5 \\ \hat{f}_{x > m} \leq 0.5 \\ \end{cases}$$

Since the data itself is a collection of random variables, any statistic
calculated from that data is also a random variable, with a probability
distribution that can be derived from the DGP.

We will focus on the sample average in an IID sample.  Let $\mu_x = E(x_i)$
and $\sigma_x^2 = var(x_i)$.  Then:
$$E(\bar{x}) = E\left( \frac{1}{n} \sum_{i=1}^n x_i\right) = \frac{1}{n} \sum_{i=1}^n E\left( x_i\right) = \frac{1}{n} \sum_{i=1}^n \mu_x = \mu_x$$

We can also calculate $var(\bar{x})$:

$$var(\bar{x}) = cov(\bar{x},\bar{x}) = E\left[\left( \frac{1}{n} \sum_{i=1}^n x_i-\mu\right)\left(\frac{1}{n} \sum_{i=1}^n x_i - \mu\right)\right] = \frac{1}{n^2} \sum_{i=1}^n\sum_{j=1}^n E\left((x_i-\mu)(x_j-\mu)\right) = \frac{1}{n^2} \sum_{i=1}^n\sum_{j=1}^n cov(x_i,x_j)$$
$$cov(x_i,x_j) = \begin{cases} \sigma_x^2 & i = j \\ 0 & i \neq j \\ \end{cases}$$ 
so:
$$var(\bar{x}) = \frac{1}{n^2} \sum_{i=1}^n \sigma_x^2 = \frac{n\sigma_x^2}{n^2} = \frac{\sigma_x^2}{n}$$

## Parameters and estimators

A ***parameter*** is an unknown number characterizing a DGP. For example,
if $x_i$ is a random sample from the $N(\mu,\sigma^2)$ distribution,
then both $\mu$ and $\sigma$ are parameters.

An ***estimator*** is a statistic that is being used to ***estimate***
(guess at the value of) an unknown parameter.  It is sometimes useful
to distinguish between 

- the ***estimand** is the thing being estimated
- the estimate is the specific number calculated from
  our particular data set.
- the estimator is either the formula used to calculate the estimate
  or the random variable that the estimate represents a draw from
  
We want estimators to be *good* guesses of the unknown parameter.
That is, we want it to have a high probability of being close to
the correct value.  

The ***error*** of the estimator relative to the parameter is
defined as:
$$err = s - \theta$$
We want the error to be as close to zero as possible.

We have a few criteria to consider.

The first is the ***bias*** of the estimator, which is defined as:
$$bias(s) = E(s - \theta) = E(s) - \theta$$
The bias represents the average error we would expect over multiple
trials. Ideally we would want $bias(s)$ to be zero, in which case
we would say that $s$ is an ***unbiased*** estimator of $\theta$.
Note that bias is always defined relative to the parameter we wish 
to estimate, and is not an inherent property of the statistic.

The second consideration is the ***variance*** of the estimator,
which is *not* defined relative to the parameter:
$$var(s) = E[(s-(E(s))^2]$$
We would want variance to be as low as possible.

A final consideration is the ***mean squared error*** of the estimator,
which is defined as:
$$MSE(s) = E[(s-\theta)^2]$$
We can do a little math and show that:
$$MSE(s) = var(s) + [bias(s)]^2$$
The MSE criterion allows us to choose a biased estimator with low variance
over an unbiased estimator with high variance.


For example, consider the sample average $\bar{x}$ in a random sample
as an estimator of $\mu_x = E(x_i)$.
$$bias(\bar{x}) = E(\bar{x}) - \mu_x = \mu_x - \mu_x = 0$$
That is, the sample average is an unbiased estimator of the population mean.

We have already found that $var(\bar{x}) = \sigma^2/n$, and we can use these
two pieces of information to calculate the mean squared error:
$$MSE(\bar{x}) = var(\bar{x}) + [bias(\bar{x})]^2 = \frac{\sigma_x^2}{n} + 0^2 = \frac{\sigma_x^2}{n}$$
This is beyond the scope of this course, but we can prove that the sample
average has a smaller MSE than any other unbiased estimator of $\mu_x$.

Here's an example of a biased estimator that has lower MSE than any unbiased
estimator.  Suppose that $x_i \sim N(\mu_x,\sigma_x^2)$ where we already 
know that $\mu_x \geq 0$.  The sample average $\bar{x}$ will typically
be a pretty good estimator, but since we know $\mu_x \geq 0$ we can construct 
a better estimator. Let $s = max(\bar{x},0)$. This will be a better estimator
since it will be the same as $\bar{x}$ whenever $\bar{x} \geq 0$ and it
will be strictly closer than $\bar{x}$ to the true value $\mu_x$ whenever
$\bar{x} < 0$. With a little bit of math we can show that
this alternative estimator is biased:
$$E(s) > E(\bar{x}) = \mu_x$$
but has lower MSE than the (unbiased) sample average:
$$MSE(s) < MSE(\bar{x})$$
## The sample frequency as an estimator

Suppose we are interested in estimating the probability of a 
particular event $x_i \in A$, where $A$ is some set.

Let $y_i$ be an indicator for whether $x_i \in A$ is true or false:
$$y_i = \begin{cases} 0 & x_i \notin A \\ 1 & x_i \in A \\ \end{cases}$$
and let $\bar{y}$ be its sample average:
$$\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i$$
Since it is a sample average, 
$$E(\bar{y}) = E(y_i) = 0*\Pr(x_i \notin A) + 1*\Pr(x_i \in A) = \Pr(x_i \in A)$$
In other words the sample frequency of an event is an unbiased estimator 
of the event's probability.

# Asymptotics

## Asymptotic versus finite sample properties

So far, we have described statistics and estimators in terms
of their probability distribution and the mean, variance
and mean squared error associated with that probability 
distribution. 

We are able to do this fairly easy with both sample averages 
and sample frequencies (which are also sample averages)
because they are sums. Unfortunately, this is not so easy 
with other statistics (e.g. sample variances, medians, 
percentiles, etc.) that are nonlinear functions of the data.

In order to deal with those statistics, we need to construct
approximations based on the ***asymptotic*** properties
of the statistics.  Asymptotic properties are properties 
that hold approximately, with the approximation getting 
closer and closer to the truth as the sample size gets 
larger. Properties that hold exactly for any sample 
size (for example, the result we established earlier that
the sample average is an unbiased estimator for the mean)
are sometimes called ***exact*** or ***finite sample***
properties.

## The law of large numbers

The ***law of large numbers*** says that for a large enough random
sample, the sample average is very likely to be very close to the
corresponding population mean.  

To get an idea of how the law of large numbers works, let's take
a look at the mean squared error of the sample average in 
a random sample:
$$MSE(\bar{x}) = \frac{\sigma_x^2}{n}$$
Notice that as $n$ gets larger and larger, $MSE(\bar{x})$ gets smaller and 
smaller.  In the language of limits:
$$\lim_{n \rightarrow \infty} MSE(\bar{x}) = \lim_{n \rightarrow \infty} \frac{\sigma_x^2}{n} = 0$$


This takes us to the ***law of large numbers*** (LLN), which is one of the 
most important results in statistics.

**LAW OF LARGE NUMBERS**: Let $\bar{x}_n$ be the sample average 
from a random sample of size $n$ on the random variable $x_i$ with
mean $E(x_i) = \mu_x$ and let $\epsilon > 0$ be any strictly positive number.  Then:
$$\lim_{n \rightarrow \infty} \Pr( |\bar{x}_n -\mu| < \epsilon) = 1$$

What does this all mean?

- Define "very close" by choosing
  - some $\epsilon > 0$ telling me how close $\bar{x}$ needs
    to be to $\mu_x$.
  - some $\delta > 0$ telling me how close $\Pr(|\bar{x}-\mu_x| < \epsilon)$
    has to be to 1.
- The LLN tells me that I can find a sample size $n$ that is 
  "big enough" to guarantee that:
  $$\Pr(|\bar{x}-\mu_x| < \epsilon) > 1-\delta$$
ANother way of saying this is that $\bar{x}_n$ ***converges in probability***
to $\mu_x$, or:
$$\bar{x}_n \rightarrow^p \mu_x$$

More generally, we say that the statistic $s$ is a ***consistent*** 
estimator of a parameter $\theta$ if:
$$s \rightarrow^P \theta$$
It will turn out that most of the statistics we use are consistent 
estimators of the thing we typically use them to estimate.

The law of large numbers is extremely powerful and important, as it
is the basis for both the casino industry, the insurance industry,
and much of the banking industry. 

A casino (or an insurance company, which is almost the same thing)
works by taking in a *large* number of *independent* small
bets. These bets have a small house advantage, so their expected 
benefit to the casino is positive. Any one bet can represent a 
sizeable loss to the casino, and if it is big enough can even 
result in the casino going bankrupt. But the LLN virtually
guarantees that the gains will outweigh the losses as long as the
casino takes in a large enough number of independent bets.

Now we could probably do without casinos, but insurance is another 
matter. The way the insurance industry works is that each of
us faces a small risk of a catastrophic cost: a house that
burns down, a car accident leading to serious injury, etc.
Insurance works by collecting a little bit of money from each
of us, and paying out a lot of money to the small number of 
people who have claims.  As with a casino, their expected
payout is less than their expected revenue, and so their
expected profits are positive.  On its own, that doesn't
mean their actual profits will be positive: positive expected
profits can still mean a high probability of big losses.
But if the insurance company covers a large number of 
*independent* risks, the law of large numbers guarantees
that the insurer will (almost) always make positive profits.

Sometimes insurance companies do lose money, and even go 
bankrupt. The usual cause of this is a big systemic 
event like a natural disaster, pandemic or financial crisis
that affects everyone.  Here the independence needed for the LLN
does not apply.



