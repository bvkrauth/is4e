---
title: 'Chapter 9: Statistical inference'
author: "ECON 233, Brian Krauth"
date: "Spring 2021"
output:
  html_document:
    number_sections: yes
    theme: paper
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter overview

In this chapter we will learn how to:

 - Construct and perform a simple hypothesis test
 - Construct a confidence interval
 - Correctly interpret both hypothesis tests and confidence intervals

# Hypothesis tests

Suppose we have a data set $D$ with unknown joint distribution $P_D$. 
Suppose we have a parameter of interest $\theta = \theta(P_D)$
whose value can in principle be determined from the DGP. 

In chapter 6, we talked about estimation, which is the use of statistics
to construct the best possible guess at the value of $\theta$.

In this chapter we will pursue a different goal.  Instead of using our
data to guess the value of $\theta$ we will use the data to ask
what values of $\theta$ we can confidently rule out.

- A hypothesis test determines whether a particular value or range of
  values can be ruled out.
  - Intuitively, if we have no (useful) data we cannot rule anything
    out.
  - As we have more data, we can rule out more and more values.
- A confidence interval determines a range of values that cannot
  be ruled out.
  - Intuitively, if we have no useful data the confidence interval
    is $(-\infty,\infty)$.
  - As we have more data, the confidence interval gets narrower 
    and narrower.
    
The set of procedures for constructing confidence intervals and 
hypothesis tests is called statistical ***inference***.

## A general framework 

We will start with hypothesis tests. 

Suppose that we have data from a local casino on the results 
from $n$ games of roulette. Specifically we have data 
$D = (x_1,\ldots,x_n)$ where 
$x_i = I(\textrm{red wins})$.  We know that for a fair roulette
wheel red wins with probability $18/37 \approx 0.486$.
But maybe the casino is cheating.  How can we determine
whether we have a fair game?  Intuitively:

  - If red wins about 48.6\% of the time, the game is probably fair.
  - If red always wins or never wins, the game is probably not fair.
  - There is some intermediate range of winning percentages 
    where we can't be sure one way or the other.
  
We need a well-justified criterion for making these decisions.

### The null and alternative hypotheses

The first step in a hypothesis test is to define the 
***null hypothesis***.  The null hypothesis is a statement
about our parameter $\theta$ that takes the form:
  $$H_0: \theta = \theta_0$$
where $\theta_0$ is a specific number. The null hypothesis
is the value of $\theta$ we are interested in ruling out.

The next step is to define the ***alternative hypothesis***. The 
alternative hypothesis defines every other value of $\theta$ 
we are allowing, and is usually written as:
  $$H_1: \theta \neq \theta_0$$
where $\theta_0$ is the same number as used in the null.

In our roulette example, our null hypothesis is:
  $$H_0: p_{red} = 18/37 \approx 0.486$$
and the alternative hypothesis is:
  $$H_1: p_{red} \neq 18/37 \approx 0.486$$

### The test statistic

Our next step is to construct a usable ***test statistic***. A
test statistic for a given null hypothesis is a statistic that
has the following two properties:

1. Its probability distribution under the null hypothesis
   (i.e., when $H_0$ is true) is known.
   - That is, it cannot depend on any other unknown features of
    the DGP $P_D$.
2. Its probability distribution under the alternative hypothesis
   (i.e., when $H_0$ is false) is different from its
   probability distribution under the null.
   - The probability distribution under the alternative is usually
      unknown because it depends on the true value of $\theta$.
      But we will be OK as long as we know it is *different.*

Most, but not all, test statistics are constructed so they are typically
close to zero when the null is true, and far from zero when the null is false.

In our roulette example, a natural test statistic would be the 
absolute sample frequency:
  $$n\hat{f}_{red} = \sum_{i=1}^n x_i$$
Since $x_i \sim Bernoulli(p_{red})$ we have:
  $$n\hat{f}_{red} \sim Binomial(n,p_{red})$$
Under the null the distribution is known:
  $$n\hat{f}_{red} \sim Binomial(n,18/37)$$
and under the alternative the distribution is different:
  $$n\hat{f}_{red} \sim Binomial(n,p) \textrm{ where $p \neq 18/37$ }$$
Intuitively, the frequency will be close to $0.486n$ when the null
is true, and far from $0.486n$ when the null is false.

### Significance and critical values

Once we have our test statistic $t$, the next thing is to choose 
***critical values***.  The critical values are two numbers
$c_L$ and $c_H$ such that:

- We ***reject the null*** if $t < c_L$ or $t > c_H$.
  - This means we conclude that $H_0$ is false.
- We ***fail to reject the null*** if $c_L < t < c_H$.
  - This does not mean we conclude that $H_0$ is true.
  - It only means that we cannot conclude that it is false.
    It may be true, or it may be false but we don't have 
    enough evidence to tell that it is false.

How do we choose critical values? We need to balance two considerations:

- We want our test to reject the null if it is false. The probability
  of rejecting a false null is called the ***power*** of the test.
    - Power is good.
    - Widening the critical range (by reducing $c_L$ or increasing
      $c_H$ will increase power, which is good.
- We do not want our test to reject the null if it is true. The 
  probability of rejecting a true null is called the ***size***
  or ***significance*** of a test.
    - Size is bad.
    - Widening the critical range (by reducing $c_L$ or increasing
      $c_H$ will increase size, which is bad.
      
Ideally, we would want to choose critical values that optimize the 
trade-off between power and size. But that isn't what we do. Instead,
we typically set the size to a fixed value $\alpha$ that is set 
by convention. 

- In economics and most other social sciences, the usual convention 
  is to use a size of 5\%. 
  - That is, we tolerate a 5\% chance of mistakenly rejecting the null
    when it is true.
  - We use this convention because it provides a reasonable amount 
    of power for the typical data set we have.
  - This typically gives a reasonable We will sometimes use a size 
    of 10\% when we don't have much data, or 1\% when we have a 
    lot of data.
- In physics, where it is possible to generate much larger data 
  sets, the conventional size is much lower. 

Once we have set our target size, we can calculate the implied 
critical values from our null distribution.

In our example, under the null:
  $$n\hat{f}_{red} \sim Binomial(n,18/37)$$
We can get a size of 5\% by choosing:
  $$c_L = 2.5 \textrm{ percentile of } Binomial(n,18/37)$$
  $$c_H = 97.5 \textrm{ percentile of } Binomial(n,18/37)$$
That is, when the null is true, there is a 2.5\% chance that 
$t < c_L$ and a 2.5\% chance that $t > c_H$, implying a 5\% 
chance of rejecting the null.

Let's assume now that $n = 100$. We can then use Excel or 
R to calculate these critical values:
```{r}
cat("2.5 percentile of binomial(100,18/37) =",
    qbinom(0.025,100,18/37),
    "\n")
cat("97.5 percentile of binomial(100,18/37) =",
    qbinom(0.975,100,18/37),
    "\n")
```
In other words we reject the null (at 5\% significance) that 
the roulette wheel is fair if red wins fewer than 39 spins 
or more than 58 spins.

### The power of a test

As mentioned above, the power of a test is defined as the probability 
of rejecting the null when it is false, and the alternative is true.

The size of a test is a number, since the distribution of the test statistic
is known under the null.  Since the alternative typically allows
more than one value of the parameter $\theta$, the power
of a test is not a number but a *function* of the unknown true value
of $\theta$ (and sometimes other unknown features of the DGP):
  $$power(\theta) = \Pr(\textrm{reject $H_0$})$$
In some cases we can actually calculate this function.

I won't make you do that, but I'll show you the power curve for our example.  A few things you should notice about it:

- The power curve reaches its lowest value 
  at the red point which corresponds to the null value for
  the parameter $\theta = 18/37$ and the size of the test
  (0.05). That is:
    - The power is always at least as big as the size,
      and is usually bigger.
    - We are more likely to reject the null when it is
      false than when it is true. That's good!
    - When a test has this desirable property, we call 
      it an ***unbiased*** test. 
- Power increases as $\theta$ gets further and further
  from the null.  
    - That is, we are more likely to detect unfairness
      in a game that is *very* unfair than when in one that
      is *a little* unfair.
- Power also increases with the sample size; the blue line
  ($n = 100$) is above the green line ($n = 20$).
  - Power analysis is often used by researchers to determine 
    how much data to collect.  Each additional observation 
    costs money (so you don't want to get too much) but
    gains power (so you don't want to get too little).

These characteristics are shared by most commonly-used tests.

```{r}
theta <- seq(0,1,length.out=100)
power100 <- pbinom(qbinom(0.025,100,18/37),100,theta) +
  (1 - pbinom(qbinom(0.975,100,18/37),100,theta))
power10 <- pbinom(qbinom(0.025,20,18/37),20,theta) +
  (1 - pbinom(qbinom(0.975,20,18/37),20,theta))
plot(theta,power100,
    type="l",
    xlab="theta",
    ylab = "power(theta)",
    main = "Power curve for fair roulette wheel",
    col = "blue")
lines(theta,power10,col="green")
abline(v=18/37,lty=2,col="gray")
abline(h=0.05,lty=2,col="gray")
points(x=18/37,y=0.05,col="red")
```

### P-values

The convention of using a conventional significance level such 
as 5\% for hypothesis tests is somewhat arbitrary and has some
negative unintended consequences:

  1. Sometimes a test statistic falls just below or just above the
    critical value, and small changes in the analysis can change 
    a result from reject to cannot-reject.
  2. In many fields, unsophisticated researchers and journal 
    editors misinterpret "cannot reject the null" as "the null is true."
    
One common response to these issues is to report what is called
the ***p-value*** of a test.  The p-value of a test is defined
as the significance level at which one would switch from rejecting
to not-rejecting the null.  For example:

- If the p-value is 0.43 (43\%) we would not reject the null at 10\%,
  5\% or 1\%.
- If the p-value is 0.06 (6\%) we would reject the null at 10\% but not
  at 5\% or 1\%.
- If the p-value is 0.02 (2\%) we would reject the null at 10\% and 5\%
  but not at 1\%.
- If the p-value is 0.001 (0.1\%) we would reject the null at 10\%, 5\%,
  and 1\%.
  
The p-value of a test is simple to calculate from the test statistic and 
its distribution under the null.  I won't go through that calculation 
here.

## Application: the mean of a random variable

Suppose that $D = (x_1,\ldots,x_n)$ is a random sample of size $n$
on some random variable $x_i$ with unknown mean $E(x_i) = \mu_x$
and variance $var(x_i) = \sigma_x^2$.  Suppose you
have calculated the sample average:
  $$\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$$
and sample variance:
  $$s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2$$

You want to test the null hypothesis:
  $$H_0: \mu_x = 0$$
against the alternative hypothesis 
  $$H_1: \mu_x \neq 0$$
Having stated our null and alternative hypotheses, we need to
construct a test statistic.

Remember that our test statistic needs to have a known distribution
under the null, and a different distribution under the alternative.
Unfortunately, if we don't know the exact probability distribution
of $x_i$ we don't know the exact probability distribution of statistics
calculated from it.  Once we have a potential test statistic, there 
are two standard solutions to this problem:
  
  1. Assume a specific probability distribution (usually a normal
    distribution) for $x_i$.  We can (or at least a 
    proefessional statistician can) then mathematically derive 
    the distribution of any test statistic from this distribution.
  2. Use the central limit theorem to get an approximate probability
    distribution.

### An asymptotic test

We will start with solution \#2. We know from the central limit 
theorem that the distribution of:
  $$z_n = \sqrt{n}\frac{\bar{x}-\mu_x}{\sigma_x}$$
is approximately $N(0,1)$ if $n$ is large enough. $z_n$ is
not a statistic because it includes the unknown parameters
$\mu_x$ and $\sigma_x$, but maybe we can find a test statistic
that is similar to $z_n$.  We proceed in two steps:

  1. We replace $\mu_x$ with its value under the null (zero).
  2. We replace $\sigma_x$ with a consistent estimator
    $s_x$.
    
This gives us the test statistic:
  $$t_n = \sqrt{n}\frac{\bar{x}_n}{s_x}$$
The central limit theorem and Slutsky theorem imply that
  $$t_n \rightarrow^D N(0,1)$$
so we can approximate the probability distribution of $t_n$ under
the null with the $N(0,1)$ distribution.

Under the alternative, we can divide $t_n$ into two components:
  $$t_n = \sqrt{n}\frac{\bar{x}_n - \mu_x}{s_x} + \sqrt{n}\frac{\mu_x}{s_x}$$
The first component converges in distribution to a $N(0,1)$,
while the second component "explodes" to either $\infty$
(if $\mu_x > 0$) or $-\infty$ (if $\mu_x < 0$). So if the
null is false, $t_n$ gets very large as $n$ gets large.

Having chosen our test statistic, we now choose critical 
values.  Under the null, $t_n$ has the approximate $N(0,1)$
distribution, so we can get critical values for a 5\% test:
  $$c_L = 2.5 \textrm{ percentile of } N(0,1) \approx -1.96$$
  $$c_H = 97.5 \textrm{ percentile of } N(0,1) \approx 1.96$$
This kind of test is so common that I want you to remember these
numbers.

### A finite-sample test

An alternative to asymptotic tests is to make specific assumptions
about the probability distribution of our DGP. Specifically,
we are going to assume that our data come from a normal distribution:
  $$x_i \sim N(\mu_x,\sigma_x^2)$$
We are also going to use the same test statistic as in our asymptotic
test:
  $$t_n = \sqrt{n}\frac{\bar{x}_n}{s_x}$$
Now at this point we should stop and note that normality of the 
data is an *assumption* that may very well be false.

Now, if this was a more advanced course I would derive the 
distribution of $t_n$ under the null.  But for ECON 233 I will
just ask you to understand that it *can* be derived once we 
assume normality of the $x_i$.

The null distribution of the test statistic $t_n$ was derived
in the 1920's by William Sealy Gosset, a statistician working 
at the Guiness brewery.  To avoid getting in trouble at work
(Guiness did not want to give away trade secrets) Gosset published
under the pseudonym "Student".  As a result, the family of 
distributions he derived is called "Student's T distribution".

More precisely, the test statistic $t_n$ as described above
has Student's T distribution with $n-1$ degrees of freedom:
  $$t_n \sim T_{n-1}$$
when the null is true.

The $T_{n-1}$ distribution looks a lot like the $N(0,1)$
distribution, but has slightly higher probability of extreme
positive or negative values.  As $n$ increases the 
$T_{n-1}$ distribution converges to the 
$N(0,1)$ distribution, just as predicted by the central 
limit theorem.

```{r}
x <- seq(-3,3,length.out = 100)
t5 <- dt(x,df=4)
t10 <- dt(x,df=9)
t30 <- dt(x,df=29)
tinf <- dnorm(x)
plot(x,tinf, type="l",col="black")
lines(x,t5,col="green")
lines(x,t10,col="blue")
lines(x,t30,col="red")
```

Having found our test statistic and its distribution under the null, 
we can calculate our critical values:
  $$c_L = 2.5 \textrm{ percentile of } T_{n-1}$$
  $$c_H = 97.5 \textrm{ percentile of } T_{n-1}$$
We can obtain these percentiles using Excel or R.

For example, if we have 5 observations, then:
```{r}
  cat("cL = 2.5 percentile of T_4 = ",
      qt(0.025,df=4),
      "\n")
  cat("cH = 97.5 percentile of T_4 = ",
      qt(0.975,df=4),
      "\n")
```
In contrast, if we have 30 observations, then:
```{r}
  cat("cL = 2.5 percentile of T_29 = ",
      qt(0.025,df=29),
      "\n")
  cat("cH = 97.5 percentile of T_29 = ",
      qt(0.975,df=29),
      "\n")
```
and if we have 1,000 observations:
```{r}
  cat("cL = 2.5 percentile of T_999 = ",
      qt(0.025,df=999),
      "\n")
  cat("cH = 97.5 percentile of T_999 = ",
      qt(0.975,df=999),
      "\n")
  cat("cL = 2.5 percentile of N(0,1) = ",
      qnorm(0.025),
      "\n")
  cat("cH = 97.5 percentile of N(0,1) = ",
      qnorm(0.975),
      "\n")
```

Notice that:

  - The test is more conservative (larger critical values, lower
    probability of rejection) for smaller sample sizes.
  - For any finite $n$ this test is at least slightly more
    conservative than the asymptotic test. However,
    - At some point (around $n = 30$) the difference between 
      the two tests becomes negligible.

In practice, most data sets in economics have well over 30
observations so economists tend to use asymptotic tests
unless they have a very small sample.


# Extensions

## Confidence intervals 

Hypothesis tests have one very important limitation: although they 
allow us to rule out $\theta = \theta_0$ for a single value of $\theta_0$, 
they say nothing about other values very close to $\theta_0$.

For example, if a medical researcher rejects the null hypothesis 
that a particular treatment has no effect on patients, that finding 
does not rule out the possibility that it has a tiny effect. 
On a similar note, if the researcher fails to reject the null of no 
effect, that does not rule out the possibility of a large effect.

The solution to this limitation is to report what are called 
***confidence intervals***.  A confidence interval is a range 
of values for $\theta$ that has been constructed from the data 
in such a way that it is very likely to contain the true value.

  - The interval $(-\infty,\infty)$ will always contain the true 
    value, but is totally uninformative.
  - A wider confidence interval is 
    - more likely to contain the true value (good).
    - less informative (bad) 
  - A narrow confidence interval is 
    - less likely to contain the true value (bad).
    - less informative (good) 

As with hypothesis testing we address this trade-off by convention.
In economics and most other social sciences the convention
is to report the ***95 \% confidence interval*** which
is defined as an interval $[s_L,s_H]$ calculated from the
data that has the property:
  $$\Pr(s_L < \theta < s_H) = 0.95$$
Now an important thing to remember is that $\theta$ is a fixed
parameter, and is not random.  The interval is the random part.

How do we calculate confidence intervals? It turns out to be 
entirely straightforward: confidence intervals can be constructed
by inverting hypothesis tests: the 95\% confidence interval for 
$\theta$ can be defined as the set of all $\theta_0$ values
that can be rejected at a 5\% level of significance.

- The 90\% confidence interval is the set of all $\theta_0$ values
  that can be rejected at a 10\% level of significance. It is 
  narrower than the 95\% confidence interval.
- The 99\% confidence interval is the set of all $\theta_0$ values
  that can be rejected at a 1\% level of significance. It is 
  wider than the 95\% confidence interval.

Confidence intervals for the mean are very easy to calculate.

Suppose we test the null
  $$H_0: \theta = \theta_0$$
Our test statistic is:
  $$t_n = \sqrt{n}\frac{\bar{x}-\theta_0}{s_x}$$
and we fail to reject the null if
  $$c_L < t_n < c_H$$
Plugging $t_n$ to this expression we get   
  $$c_L < \sqrt{n}\frac{\bar{x}-\theta_0}{s_x} < c_H$$
Solving for $\theta_0$ we fail to reject if:
  $$\bar{x} - c_H s_x/\sqrt{n} < \theta_0 < \bar{x} - c_L s_x/\sqrt{n}$$

If we are using the asymptotic approximation to construct
a 95\% confidence interval, then the 5\% asyptotic critical values
are $c_L = -1.96$ and $c_H \approx 1.96$ and the 
confidence interval is:
  $$CI = \bar{x} \pm 1.96 s_x/\sqrt{n}$$
In other words, the 95\% confidence interval for $\mu_x$ 
is just the point estimate plus or minus roughly 2 standard
errors.  

If we have a small sample, and choose to assume normality rather
than using the asymptotic approximation, then we need to use the
slighly larger critical values from the $T_{n-1}$ distribution.
For example, if $n=5$, then $c_L \approx -2.78$,
$c_H \approx 2.78$ and the 95\% confidence interval is:
  $$CI = \bar{x} \pm 2.78 s_x/\sqrt{n}$$
As with hypothesis tests, finite sample confidence intervals
are typically more conservative than their asymptotic
cousins, but the difference becomes negligible as 
the sample size increases.

Returning to our roulette example, suppose that $n = 100$ and red wins in 40 cases. Then the 95\% confidence interval for $\theta$ is:

```{r}
  theta <- seq(0,1,length.out=100)
  pval <- 2*pmin(pbinom(40,100,theta),
                   (1 - pbinom(40,100,theta)))
  # plot(theta,pval)
  cat("95 % CI for nf=40: ",
      range(theta[pval > 0.05]),
      "\n")
```

In other words:

  - The true probability of red winning has a 95\% chance
    of being between 31.3\% and 49.5\%. We cannot reject
    any value in that range at a significance level of 5\%.
  - We can reject at 5\% any true probability less than 31.3\%
    or greater than 49.5\%.
    
Unfortunately, while I can do this particular calculation it is
more challenging than we need for this course.

Fortunately, some other confidence intervals can be easily worked 
out by pen and paper, and we will move on to those next.

## One-tailed tests and confidence intervals

Our simple testing setup assumed we wanted to test a single
point null:
  $$H_0: \theta = \theta_0$$
against all alternatives:
  $$H_1: \theta \neq \theta_0$$
The resulting test procedure features both an upper critical value
and a lower critical value.

But there are many applications in which we are only interested
in some alternatives. For example,

  - In medical applications we usually want to know if a proposed
    treatment has a *beneficial* effect.  Knowing that the treatment
    has a nonzero effect is not particularly helpful, because that
    could mean it *harms* the patients.
  - In economic applications, we can often rule out some values for
    parameters of interest. For example, we know that demand curves
    slope down: this means that their slope (or elasticity) is always
    zero or negative, and never positive.

The solution to this is to perform what is called a **one-tailed***
test.  A one-tailed test differs from a standard two-tailed test
in two ways:

1. The alternative hypothesis is either:
    $$H_1: \theta > \theta_0$$
    or 
    $$H_1: \theta < \theta_0$$
    That is, we are ruling out some values of $\theta$ as either
    impossible (as in example \#2 above) or simply uninteresting
    (as in example \#1 above).
2. We eliminate one of the critical values, and adjust the other 
    critical value so that the size is at the desired level.
    That is, either:
    $$c_L = -\infty$$
    $$c_H = 95th \textrm{ percentile of the null distribution}$$
    or
    $$c_L = 5th \textrm{ percentile of the null distribution}$$
    $$c_H = \infty$$

    
For example, suppose that we are interested in testing:
  $$H_0: \mu_x = 0$$
  $$H_1: \mu_x > 0$$
according to the usual asymptotic test at 5\% significance.

Then our critical values are
  $$c_L = -\infty$$
  $$c_H = \Phi^{-1}(0.95) \approx 1.645 $$
That is, we reject the null in favor of the alternative if 
$t_n > 1.645$, and fail to reject the null otherwise.

Why would you use a one-tailed test?

  - By restricting the scope of the alternative, we gain
    power for every value of $\theta$ in this restricted 
    alternative.
  - The cost of this gain in power is that the test is biased
    if our restrictions are invalid (i.e., if the true $\theta$
    is actually less than $\theta_0$)

We can also invert one-tailed hypothesis tests to generate one-tailed
confidence intervals. Continuing on with our mean example, the 
one-tailed 95\% asymptotic confidence interval for $\mu_x$ 
is either:
  $$(-\infty,\bar{x}+1.645 s_x/\sqrt{n})$
or
  $$(\bar{x} - 1.645 s_x/\sqrt{n}, \infty)$
depending on which tail we want.


# Things I have skipped

## Testing linear restrictions on multiple parameters  

## Estimating the variance and standard deviation

## Testing multiple linear restrictions
