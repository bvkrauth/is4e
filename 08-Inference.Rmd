# Statistical inference

Chapter 6 covered estimation, which is the use of data and statistics
to construct the best possible guess at the value of some parameter
$\theta$.

In this chapter, we will pursue a different goal.  Instead of guessing
the value of $\theta$ we will use the data to ask what values of 
$\theta$ can be confidently ruled out.

- A hypothesis test determines whether a particular value can be ruled 
  out.
- A confidence interval determines a range of values that cannot
  be ruled out.

The set of procedures for constructing confidence intervals and 
hypothesis tests is called statistical ***inference***.

::: goals
**Chapter goals**

In this chapter we will learn how to:

 - Construct and perform a simple hypothesis test
 - Construct a confidence interval
 - Correctly interpret both hypothesis tests and confidence intervals
:::

## Principles of inference

We start by describing a very general framework, using our roulette
example to make the abstract ideas more concrete.  

In the general framework, we have a data set $D$ with unknown 
joint distribution $P_D$. We also have a parameter of interest 
$\theta = \theta(P_D)$ whose value can in principle be determined
from the DGP. 

::: example
Suppose that we have data from a local casino on the results 
from $n=100$ games of roulette. Specifically we have data 
$D = (x_1,\ldots,x_{100})$ where $x_i = I(\textrm{red wins})$.
Our parameter of interest is the probability that
red wins:
  $$p_{red} = \Pr(x_i = 1)$$

We know that red wins in a fair game with probability
$p_{red} = 18/37 \approx 0.486$. But maybe the casino is 
cheating.  How can we determine whether we have a fair game?
Intuitively:

  - If red wins about 48.6\% of the time, the game is probably fair.
  - If red always wins or never wins, the game is probably not fair.
  - There is some intermediate range of winning percentages 
    where we can't be sure one way or the other.
  
We need a well-justified criterion for making these decisions.
:::

### Hypothesis tests

We will start with hypothesis tests. The idea of a hypothesis test
is to determine whether the data rule out or ***reject*** a specific
value of the unknown parameter $\theta$.

Intuitively, if we have no (useful) data we cannot rule anything
out, but as we obtain more data, we can rule out more values.

#### The null and alternative hypotheses

The first step in a hypothesis test is to define the 
***null hypothesis***.  The null hypothesis is a statement
about our parameter $\theta$ that takes the form:
  $$H_0: \theta = \theta_0$$
where $\theta_0$ is a specific number. The null hypothesis
is the value of $\theta$ we are interested in ruling out.

The next step is to define the ***alternative hypothesis***. The 
alternative hypothesis defines every other value of $\theta$ 
we are allowing, and is usually written as:
  $$H_1: \theta \neq \theta_0$$
where $\theta_0$ is the same number as used in the null.

::: example
In our roulette example, our null hypothesis is
that the wheel is fair:
  $$H_0: p_{red} = 18/37 \approx 0.486$$
and the alternative hypothesis is that it is not fair:
  $$H_1: p_{red} \neq 18/37 \approx 0.486$$
:::

#### The test statistic

Our next step is to construct a usable ***test statistic***. A
test statistic for a given null hypothesis is a statistic that
has the following two properties:

1. Its probability distribution under the null 
   (i.e., when $H_0$ is true) is *known*.
2. Its probability distribution under the alternative 
   (i.e., when $H_0$ is false) is *different* from its
   probability distribution under the null.

Most, but not all, test statistics are constructed so they are typically
close to zero when the null is true, and far from zero when the null is false.

::: example
In our roulette example, a natural test statistic would be the 
absolute sample frequency:
  $$n\hat{f}_{red} = \sum_{i=1}^n x_i$$
Since $x_i \sim Bernoulli(p_{red})$ we have:
  $$n\hat{f}_{red} \sim Binomial(n,p_{red})$$
Under the null the distribution is known:
  $$n\hat{f}_{red} \sim Binomial(100,18/37)$$
and under the alternative the distribution is different:
  $$n\hat{f}_{red} \sim Binomial(100,p_{red}) \textrm{ where $p_{red} \neq 18/37$ }$$
Notice that the distribution under the alternative is not known,
since $p_{red}$ is not known.  That's OK, it only needs to be *known* under
the null and *different* under the alternative.
:::

#### Significance and critical values

Once we have our test statistic $t$, the next thing is to choose 
***critical values***.  The critical values are two numbers
$c_L$ and $c_H$ such that:

- We ***reject the null*** if $t < c_L$ or $t > c_H$.
  - This means we conclude that $H_0$ is false.
- We ***fail to reject the null*** if $c_L < t < c_H$.
  - This does not mean we conclude that $H_0$ is true.
  - It only means that we cannot conclude that it is false.
    It may be true, or it may be false but we don't have 
    enough evidence to tell that it is false.

How do we choose critical values? We need to balance two considerations:

- We want our test to reject the null when it is false. The probability
  of rejecting a false null is called the ***power*** of the test.
    - Power is good.
    - Widening the critical range (by reducing $c_L$ or increasing
      $c_H$ will increase power, which is good.
- We do not want our test to reject the null when it is true. The 
  probability of rejecting a true null is called the ***size***
  or ***significance*** of a test.
    - Size is bad.
    - Widening the critical range (by reducing $c_L$ or increasing
      $c_H$ will increase size, which is bad.
      
Ideally, we would want to choose critical values that optimize the 
trade-off between power and size. But that isn't what we do. Instead,
we typically:

1. Set the size to a fixed value $\alpha$.
   - In economics and most other social sciences, the usual convention 
     is to use a size of 5\%. 
     - That is, we tolerate a 5\% chance of mistakenly rejecting the null
       when it is true.
     - We use this convention because it provides a reasonable amount 
      of power for the typical data set we have.
     - This typically gives a reasonable We will sometimes use a size 
      of 10\% when we don't have much data, or 1\% when we have a 
      lot of data.
   - In physics or genetics, where data sets are much larger, the 
     conventional size is much lower. 
2. Calculate critical values that imply the desired size.  For
   example, when with a size of 5\% $(\alpha = 0.05)$, we would:
   - set $c_L$ to the 2.5 percentile of the null distribution
   - set $c_H$ to the 97.5 percentile of the null distribution

::: example
In our example, under the null:
  $$n\hat{f}_{red} \sim Binomial(100,18/37)$$
We can get a size of 5\% by choosing:
  $$c_L = 2.5 \textrm{ percentile of } Binomial(100,18/37)$$
  $$c_H = 97.5 \textrm{ percentile of } Binomial(100,18/37)$$
We can then use Excel or R to calculate these critical values:
```{r criticalvalues, echo = FALSE}
cat("2.5 percentile of binomial(100,18/37) =",
    qbinom(0.025,100,18/37),
    "\n")
cat("97.5 percentile of binomial(100,18/37) =",
    qbinom(0.975,100,18/37),
    "\n")
```
In other words we reject the null (at 5\% significance) that 
the roulette wheel is fair if red wins fewer than 39 spins 
or more than 58 spins.
:::

The table below summarizes where we stand:

| Test component         | General case                         | Roulette example |
|------------------------|--------------------------------------|---------------|
| Parameter              | $p = \Pr(\textrm{some event})$       | $p = \Pr(RED)$ |
| Null hypothesis        | $H_0:p = p_0$                        | $H_0:p = 18/37$ | 
| Alternative hypothesis | $H_1: p \neq p_0$                    | $H_1: p \neq 18/37$ |
| Test statistic         | $t = n\hat{f}_{\textrm{event}}$      | $t = n\hat{f}_{RED}$ | 
| Null distribution      | $Binomial(n,p_0)$                    | $Binomial(100,18/37)$ |
| Critical value $c_L$   | 2.5 percentile of $Binomial(n,p_0)$  | 39 |
| Critical value $c_H$   | 97.5 percentile of $Binomial(n,p_0)$ | 58 |
| Decision               | Reject if $t \notin [c_L,c_H]$       | Reject if $t \notin [39,58]$ |


#### The power of a test

As mentioned above, the power of a test is defined as the probability 
of rejecting the null when it is false, and the alternative is true.

The size of a test is a number, since the distribution of the test statistic
is known under the null.  Since the alternative typically allows
more than one value of the parameter $\theta$, the power
of a test is not a number but a *function* of the unknown true value
of $\theta$ (and sometimes other unknown features of the DGP):
  $$power(\theta) = \Pr(\textrm{reject $H_0$})$$
In some cases we can actually calculate this function.

::: example
I won't make you do that, but I'll show you the power curve for our example.  
```{r power_curves, echo = FALSE}
theta <- seq(0,1,length.out=100)
power100 <- pbinom(qbinom(0.025,100,18/37),100,theta) +
  (1 - pbinom(qbinom(0.975,100,18/37),100,theta))
power10 <- pbinom(qbinom(0.025,20,18/37),20,theta) +
  (1 - pbinom(qbinom(0.975,20,18/37),20,theta))
plot(theta,power100,
    type="l",
    xlab="theta",
    ylab = "power(theta)",
    main = "Power curve for fair roulette wheel",
    col = "blue")
lines(theta,power10,col="green")
abline(v=18/37,lty=2,col="gray")
abline(h=0.05,lty=2,col="gray")
points(x=18/37,y=0.05,col="red")
```
A few things you should notice about it:

- The power curve reaches its lowest value 
  at the red point which corresponds to the null value for
  the parameter $\theta = 18/37$ and the size of the test
  (0.05). That is:
    - The power is always at least as big as the size,
      and is usually bigger.
    - We are more likely to reject the null when it is
      false than when it is true. That's good!
    - When a test has this desirable property, we call 
      it an ***unbiased*** test. 
- Power increases as $\theta$ gets further and further
  from the null.  
    - That is, we are more likely to detect unfairness
      in a game that is *very* unfair than when in one that
      is *a little* unfair.
- Power also increases with the sample size; the blue line
  ($n = 100$) is above the green line ($n = 20$).
  - Power analysis is often used by researchers to determine 
    how much data to collect.  Each additional observation 
    costs money (so you don't want to get too much) but
    gains power (so you don't want to get too little).

These characteristics are shared by most commonly-used tests.
:::

::: fyi
**P values**

The convention of always using a 5\% significance level for 
hypothesis tests is somewhat arbitrary and has some negative 
unintended consequences:

  1. Sometimes a test statistic falls just below or just above the
    critical value, and small changes in the analysis can change 
    a result from reject to cannot-reject.
  2. In many fields, unsophisticated researchers and journal 
    editors misinterpret "cannot reject the null" as "the null is true."
    
One common response to these issues is to report what is called
the ***p-value*** of a test.  The p-value of a test is defined
as the significance level at which one would switch from rejecting
to not-rejecting the null.  For example:

- If the p-value is 0.43 (43\%) we would not reject the null at 10\%,
  5\% or 1\%.
- If the p-value is 0.06 (6\%) we would reject the null at 10\% but not
  at 5\% or 1\%.
- If the p-value is 0.02 (2\%) we would reject the null at 10\% and 5\%
  but not at 1\%.
- If the p-value is 0.001 (0.1\%) we would reject the null at 10\%, 5\%,
  and 1\%.
  
The p-value of a test is simple to calculate from the test statistic and 
its distribution under the null.  I won't go through that calculation 
here.
:::

### Confidence intervals 

Hypothesis tests have one very important limitation: although they 
allow us to rule out $\theta = \theta_0$ for a single value of $\theta_0$, 
they say nothing about other values very close to $\theta_0$.

For example, if a medical researcher rejects the null hypothesis 
that a particular treatment has no effect on patients, that finding 
does not rule out the possibility that it has a tiny effect. 
On a similar note, if the researcher fails to reject the null of no 
effect, that does not rule out the possibility of a large effect.

The solution to this limitation is to report what are called 
***confidence intervals***.  A confidence interval is a range 
of values for $\theta$ that has been constructed from the data 
in such a way that it is very likely to contain the true value.

  - The interval $(-\infty,\infty)$ will always contain the true 
    value, but is totally uninformative.
  - A wider confidence interval is 
    - more likely to contain the true value (good).
    - less informative (bad) 
  - A narrow confidence interval is 
    - less likely to contain the true value (bad).
    - less informative (good) 

As with hypothesis testing we address this trade-off by convention.
In economics and most other social sciences the convention
is to report the ***95 \% confidence interval*** which
is defined as an interval $[s_L,s_H]$ calculated from the
data that has the property:
  $$\Pr(s_L < \theta < s_H) = 0.95$$
Now an important thing to remember is that $\theta$ is a fixed
parameter, and is not random.  The interval is the random part.

How do we calculate confidence intervals? It turns out to be 
entirely straightforward: confidence intervals can be constructed
by inverting hypothesis tests: 

- the 95\% confidence interval for 
  $\theta$ can be defined as the set of all $\theta_0$ values
  that can be rejected at a 5\% level of significance.
- The 90\% confidence interval is the set of all $\theta_0$ values
  that can be rejected at a 10\% level of significance. It is 
  narrower than the 95\% confidence interval.
- The 99\% confidence interval is the set of all $\theta_0$ values
  that can be rejected at a 1\% level of significance. It is 
  wider than the 95\% confidence interval.

::: example
Calculating a confidence interval for $p_{red}$ is somewhat 
tricky to do by hand, but easy to do on a computer:

1. Construct a grid of many values between 0 and 1.
2. For each value $p_0$ in the grid, test the null hypothesis
   $H_0: p_{red} = p_0$ against the alternative hypothesis
   $H_1: p_{red} \neq p_0$.
3. The confidence interval is the range of values for $p_0$
   that are not rejected.

For example, suppose that red wins on 40 of the 100 games. Then 
a 95\% confidence interval for $p_{red}$ is:
```{r rouletteCI, echo = FALSE}
theta <- seq(0,1,length.out=101)
cL <- qbinom(0.025,100,theta)
cH <- qbinom(0.975,100,theta)
thetaCI <- range(theta[cL < 40 & cH > 40])
cat(thetaCI[1]," to ",thetaCI[2],"\n")
```
Notice that the confidence interval includes the fair value of $0.486$
but it also includes some very unfair values.  In other words, while
we are unable to rule out the possibility that we have a fair game,
the evidence that we do is not actually very strong.
:::

## Application: the mean

Having described the general framework and a single example, we now
move on to the most common application: constructing hypothesis
tests and confidence intervals on the mean in a random sample.

Let $D = (x_1,\ldots,x_n)$ be a random sample of size $n$
on some random variable $x_i$ with unknown mean $E(x_i) = \mu_x$
and variance $var(x_i) = \sigma_x^2$.  

Let the sample average be:
  $$\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$$
and the sample variance:
  $$s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2$$
Both of these statistics are easily calculated from the data,
and we have previosly discussed their properties in detail.

Suppose that you want to test the null hypothesis:
  $$H_0: \mu_x = 0$$
against the alternative hypothesis 
  $$H_1: \mu_x \neq 0$$
Having stated our null and alternative hypotheses, we need to
construct a test statistic.

Remember that our test statistic needs to have a known distribution
under the null, and a different distribution under the alternative.

Unfortunately, if we don't know the exact probability distribution
of $x_i$ we don't know the exact probability distribution of statistics
calculated from it.  Once we have a potential test statistic, there 
are two standard solutions to this problem:
  
  1. Assume a specific probability distribution (usually a normal
    distribution) for $x_i$.  We can (or at least a 
    proefessional statistician can) then mathematically derive 
    the distribution of any test statistic from this distribution.
  2. Use the central limit theorem to get an approximate probability
    distribution.

### An asymptotic test

We will start with solution \#2. 

We do not know the exact probability distribution of $\bar{x}$, but 
we know from the central limit theorem that the distribution of:
  $$z_n = \sqrt{n}\frac{\bar{x}-\mu_x}{\sigma_x}$$
is approximately $N(0,1)$ if $n$ is large enough. It would be natural
to base our test statistic on $z_n$, since its (approximate)
distribution is known.

Unfotunately, $z_n$ is not a statistic because it includes the 
unknown parameters $\mu_x$ and $\sigma_x$, but maybe we can find
a test statistic that is similar to $z_n$.  

We proceed in two steps:

  1. Replace $\sigma_x$ with its consistent estimator $s_x$, 
     the sample standard deviation.
  2. Replace $\mu_x$ with its value under the null $\mu_0$.
    
This gives us the test statistic:
  $$t_n = \sqrt{n}\frac{\bar{x}_n-\mu_0}{s_x}$$
This really is a statistic, since we can calculate it from our
data.

We can divide $t_n$ into two components:
  $$t_n = \sqrt{n}\frac{\bar{x}_n - \mu_x}{s_x} + \sqrt{n}\frac{\mu_x-\mu_0}{s_x}$$
The first component is very similar to our $z_n$.  In fact,
I can prove using the CLT and Slutsky's theorem that 
  $$\sqrt{n}\frac{\bar{x}_n-\mu_x}{s_x} \rightarrow^D N(0,1)$$
The proof is well beyond the scope of this class, so I will
ask you to take it for granted.

When the null is true, the second component is zero, so the
asymptotic distribution is known under the null:
  $$t_n \rightarrow^D N(0,1)$$
When the null is false, the second component "explodes"
as $n$ increases since it is $\sqrt{n}$ times a roughly
constant number times. That is, the approximate distribution
is different under the alternative.

Having chosen our test statistic, we now choose critical 
values.  Under the null, $t_n$ has the approximate $N(0,1)$
distribution, so we can get critical values for a 5\% test:
  $$c_L = 2.5 \textrm{ percentile of } N(0,1) \approx -1.96$$
  $$c_H = 97.5 \textrm{ percentile of } N(0,1) \approx 1.96$$
This kind of test is so common that I want you to remember these
numbers.

### A finite-sample test

Most economic data comes in sufficiently large samples that the asymptotic
distribution of $t_n$ is a reasonable approximation and the asymptotic
test works well. But occasionally we have samples that are small enough
that it doesn't.  

Specifically, we are going to add the assumption that our data
comes from a normal distribution:
  $$x_i \sim N(\mu_x,\sigma_x^2)$$
We are also going to use the same test statistic as in our asymptotic
test:
  $$t_n = \sqrt{n}\frac{\bar{x}_n - \mu_0}{s_x}$$
Now at this point we should stop and note that normality of the 
data is an *assumption* that may very well be false.

If this was a more advanced course I would derive the 
distribution of $t_n$ under the null.  But for ECON 233 I will
just ask you to understand that it *can* be derived once we 
assume normality of the $x_i$.

The null distribution of this particular test statistic 
under these particular assumptions was derived
in the 1920's by William Sealy Gosset, a statistician working 
at the Guiness brewery.  To avoid getting in trouble at work
(Guiness did not want to give away trade secrets) Gosset published
under the pseudonym "Student".  As a result, the family of 
distributions he derived is called "Student's T distribution".

More precisely, the test statistic $t_n$ as described above
has Student's T distribution with $n-1$ degrees of freedom:
  $$t_n \sim T_{n-1}$$
when the null is true.

The $T_{n-1}$ distribution looks a lot like the $N(0,1)$
distribution, but has slightly higher probability of extreme
positive or negative values.  As $n$ increases the 
$T_{n-1}$ distribution converges to the 
$N(0,1)$ distribution, just as predicted by the central 
limit theorem.

```{r tdistribution, echo=FALSE}
x <- seq(-3,3,length.out = 100)
t5 <- dt(x,df=4)
t10 <- dt(x,df=9)
t30 <- dt(x,df=29)
tinf <- dnorm(x)
plot(x,tinf, type="l",col="black")
lines(x,t5,col="green")
lines(x,t10,col="blue")
lines(x,t30,col="red")
```

Having found our test statistic and its distribution under the null, 
we can calculate our critical values:
  $$c_L = 2.5 \textrm{ percentile of } T_{n-1}$$
  $$c_H = 97.5 \textrm{ percentile of } T_{n-1}$$
We can obtain these percentiles using Excel or R.

For example, if we have 5 observations, then:
```{r}
  cat("cL = 2.5 percentile of T_4 = ",
      qt(0.025,df=4),
      "\n")
  cat("cH = 97.5 percentile of T_4 = ",
      qt(0.975,df=4),
      "\n")
```
In contrast, if we have 30 observations, then:
```{r}
  cat("cL = 2.5 percentile of T_29 = ",
      qt(0.025,df=29),
      "\n")
  cat("cH = 97.5 percentile of T_29 = ",
      qt(0.975,df=29),
      "\n")
```
and if we have 1,000 observations:
```{r}
  cat("cL = 2.5 percentile of T_999 = ",
      qt(0.025,df=999),
      "\n")
  cat("cH = 97.5 percentile of T_999 = ",
      qt(0.975,df=999),
      "\n")
  cat("cL = 2.5 percentile of N(0,1) = ",
      qnorm(0.025),
      "\n")
  cat("cH = 97.5 percentile of N(0,1) = ",
      qnorm(0.975),
      "\n")
```

Notice that:

  - The test is more ***conservative** (less likely to reject)
    for smaller sample sizes.
  - For any finite sample size this test is at least slightly more
    conservative than the asymptotic test. However,
    - At some point (around $n = 30$) the difference between 
      the two tests becomes negligible.

In practice, most data sets in economics have well over 30
observations so economists tend to use asymptotic tests
unless they have a very small sample.

### Confidence intervals for the mean

Confidence intervals for the mean are very easy to calculate. Again
we construct them by inverting the hypothesis test.

Pick any $\mu_0$.  To test the null 
  $$H_0: \mu_x = \mu_0$$
our test statistic is:
  $$t_n = \sqrt{n}\frac{\bar{x}-\mu_0}{s_x}$$
and we fail to reject the null if
  $$c_L < t_n < c_H$$
where $c_L$ and $c_H$ are our critical values.

Plugging $t_n$ to this expression we fail to reject the null whenever:
  $$c_L < \sqrt{n}\frac{\bar{x}-\mu_0}{s_x} < c_H$$
Solving for $\mu_0$ we fail to reject whenever:
  $$\bar{x} - c_H s_x/\sqrt{n} < \theta_0 < \bar{x} - c_L s_x/\sqrt{n}$$
All that remains is to choose a confidence/size level, and 
decide whether to use an asymptotic or finite sample test.

If we are using the asymptotic approximation to construct
a 95\% confidence interval, then the 5\% asyptotic critical values
are $c_L = -1.96$ and $c_H \approx 1.96$ and the 
confidence interval is:
  $$CI = \bar{x} \pm 1.96 s_x/\sqrt{n}$$
In other words, the 95\% confidence interval for $\mu_x$ 
is just the point estimate plus or minus roughly 2 standard
errors.  

If we have a small sample, and choose to assume normality rather
than using the asymptotic approximation, then we need to use the
slighly larger critical values from the $T_{n-1}$ distribution.
For example, if $n=5$, then $c_L \approx -2.78$,
$c_H \approx 2.78$ and the 95\% confidence interval is:
  $$CI = \bar{x} \pm 2.78 s_x/\sqrt{n}$$
As with hypothesis tests, finite sample confidence intervals
are typically more conservative than their asymptotic
cousins, but the difference becomes negligible as 
the sample size increases.

## Extensions

### One-tailed tests and confidence intervals

Our simple testing setup assumed we wanted to test a single
point null:
  $$H_0: \theta = \theta_0$$
against all alternatives:
  $$H_1: \theta \neq \theta_0$$
The resulting test procedure features both a lower critical value
$c_L$ and an upper critical value $c_H$.

But there are many applications in which we are only interested
in some alternatives. For example,

  - In medical applications we usually want to know if a proposed
    treatment has a *beneficial* effect.  Knowing that the treatment
    has a nonzero effect is not particularly helpful, because that
    could mean it *harms* the patients.
  - In economic applications, we can often rule out some values for
    parameters of interest. For example, we know that demand curves
    slope down: this means that their slope (or elasticity) is always
    zero or negative, and never positive.

The solution to this is to perform what is called a ***one-tailed***
test.  A one-tailed test differs from a standard two-tailed test
in two ways:

1. The alternative hypothesis is either:
    $$H_1: \theta > \theta_0$$
    or 
    $$H_1: \theta < \theta_0$$
    That is, we are ruling out some values of $\theta$ as either
    impossible (as in example \#2 above) or simply uninteresting
    (as in example \#1 above).
2. We eliminate one of the critical values, and adjust the other 
    critical value so that the size is at the desired level.
    For example the critical values for a one tailed test at 
    5\% significance are either:
    $$c_L = -\infty$$
    $$c_H = 95th \textrm{ percentile of the null distribution}$$
    or
    $$c_L = 5th \textrm{ percentile of the null distribution}$$
    $$c_H = \infty$$

Exactly which critical value should be dropped is usually clear 
from the context.

::: example
Suppose that we are interested in testing:
  $$H_0: \mu_x = 0$$
  $$H_1: \mu_x > 0$$
according to the usual asymptotic test at 5\% significance.

Then our critical values are
  $$c_L = -\infty$$
  $$c_H = \textrm{95th percentile of the } N(0,1) \textrm{ distribution} = \Phi^{-1}(0.95) \approx 1.645 $$
That is, we reject the null in favor of the alternative if 
$t_n > 1.645$, and fail to reject the null otherwise.
:::

Why would you use a one-tailed test?

  - By restricting the scope of the alternative, we gain
    power for every value of $\theta$ in this restricted 
    alternative.
  - The cost of this gain in power is that the test is biased
    if our restrictions are invalid (i.e., if the true $\theta$
    is actually less than $\theta_0$)

We can also invert one-tailed hypothesis tests to generate one-tailed
confidence intervals. 


::: example
Continuing on with our mean example, the 
one-tailed 95\% asymptotic confidence interval for $\mu_x$ 
is either:
  $$(-\infty,\bar{x}+1.645 s_x/\sqrt{n})$$
or
  $$(\bar{x} - 1.645 s_x/\sqrt{n}, \infty)$$
depending on which tail we want.
:::

### Additional applications

I have focused on the principles of inference and one important 
application for testing the value of a single parameter:

- We can test a linear restriction on a set of parameters, for example
  a null hypothesis of the form:
  $$H_0: \mu_x = \mu_y$$
  where $\mu_x=E(x_i)$ and $\mu_y=E(y_i)$ are the mean of two
  different random variables
- We can test multiple hypotheses, for example a null hypothesis
  of the form:
  $$H_0: \mu_x = a \textrm{ and } \mu_y=b$$
- We can test nonlinear hypotheses, for example:
  $$H_0: \mu_x\mu_y = 1$$
All of these are beyond the scope of this course, but it is useful
to know that they are possible.


