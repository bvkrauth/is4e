# Data and sampling

We have developed some basic theory of probability, random events, and random
variables.  We have also learned some techniques for cleaning data and 
calculating statistics in Excel.  The next step is to bring these two strands
of concepts together.

Over the next two chapters we will develop a framework for talking about data
and the statistics calculated from that data as a random process that can be
described using the theory of probability and random variables.

Suppose we have a data set. We would like to use that data set to learn
something about the mechanisms by which the data set was generated.
This process of learning from data is statistical analysis.

The basic idea of statistical analysis is that:

- Each number in our data set can be thought of as a distinct random 
  variable derived from some underlying (unobserved) random outcome.
  - The random process that produced our data is called the 
    *data generating process* or DGP.
  - We have a probabilistic model of the DGP, but that model will typically 
    include some unknown numbers called *parameters* of the model.
- Each statistic we calculate from our data, since it is calculated from
  these random variables, is also a random variable.  
  - The probability distribution of each statistic could be derived from 
    the DGP *if we knew the DGP*.
- We will use some statistics as *estimates* of unknown parameters of
  the DGP.
  - We will want to use statistics that have a high probability of 
    prividing provide accurate estimates. 

Given this overall strategy, we will next talk about the details.

::: goals
***Chapter goals***

In this chapter we will:

- Calculate and interpret joint, marginal and conditional distributions.
- Calculate and interpret the covariance and correlation coefficient of 
  two discrete random variables.
- Model the random process generating a data set.
- Apply and interpret the assumption of simple random sampling, and compare
  it to other sampling schemes.
:::

## Multiple random variables

Almost all interesting data sets have multiple observations and multiple
variables and observations. So before we start talking about data, we 
need to develop some tools and terminology for thinking about multiple
random variables.

To keep things simple, most of the definitions and examples will be stated in
terms of *two* random variables.  The extension to more than two random 
variables is conceptually straightforward but will be skipped.

### Joint distributions

Let $x = x(b)$ and $y = y(b)$ be two random variables defined in 
terms of the same underlying outcome $b$.

Their ***joint probability distribution*** assigns a probability
to every event that can be defined in terms of $x$ and $y$,
for example $\Pr(x = 6 \cap y = 0)$ or $\Pr(x < y)$.

This joint distribution can be fully described by the ***joint CDF***:
$$F_{x,y}(a,b) = \Pr(x \leq a \cap y \leq b)$$
or by the ***joint PDF***:
$$f_{x,y}(a,b) = \begin{cases}
      \Pr(x = a \cap y = b) & \textrm{if $x$ and $y$ are discrete} \\
      \frac{\partial F_{x,y}(a,b)}{\partial a \partial b} & \textrm{if $x$ and $y$ are continuous} \\
      \end{cases}$$

The joint distribution tells you two things about these variables

1. The ***marginal*** distributions of the two individual random variables
   - For example, we can derive each variable's CDF from the joint CDF:
      $$F_x(a) = \Pr(x \leq a) = \Pr(x \leq a \cap y \leq \infty) = F_{x,y}(a,\infty)$$
      $$F_y(b) = \Pr(y \leq n) = \Pr(x \leq \infty \cap y \leq b) = F_{x,y}(\infty,b)$$
   - We can also derive each variable's PDF from the joint PDF
2. The *relationship* between the two variables.  
   - We will develop several ways of describing this relationship.

Note that while you can always derive the marginal distributions from the joint 
distribution, you cannot go the other way around unless you know everything
about the relationship between the two variables.

::: example
**The joint PDF in roulette**

In our roulette example, the joint PDF of $w_{red}$ and 
$w_{14}$ can be derived from the original outcome.

If $b=14$, then both red and 14 win:
  \begin{align}
    f_{red,14}(1,35) &= \Pr(w_{red}=1 \cap w_{14} = 35) \\
      &= \Pr(b \in \{14\}) = 1/37
  \end{align}
If $b \in Red$ but $b \neq 14$, then red wins but 14 loses:
  \begin{align}
    f_{red,14}(1,-1) &= \Pr(w_{red} = 1 \cap w_{14} = -1) \\
      &= \Pr\left(b \in \left\{
        \begin{gathered}
          1,3,5,7,9,12,16,18,19,21,\\
          23,25,27,30,32,34,36
        \end{gathered}\right\}\right)  \\
      &= 17/37
  \end{align}
Otherwise both red and 14 lose:
  \begin{align}
    f_{red,14}(-1,-1) &= \Pr(w_{red} = -1 \cap w_{14} = -1) \\
      &= \Pr\left(b \in \left\{
        \begin{gathered}
          0,2,4,6,7,10,11,13,15,17, \\
          20,22,24,26,28,31,33,35
        \end{gathered}\right\}\right)  \\
      &= 19/37
  \end{align}
All other values have probability zero.
:::

### Conditional distributions

The ***conditional distibution*** of a random variable $y$ given another random
variable $x$ assigns values to all probabilities of the form:
  $$\Pr(y \in A| x \in B) = \frac{\Pr(y \in A \cap x \in B)}{\Pr(x \in B)}$$
Since it is just the ratio of the joint probability to the marginal probability, the
conditional distribution can always be derived from the joint distribution.

We can describe a conditional distribution with either the ***conditional CDF***:
  $$F_{y|x}(a,b) = \Pr(y \leq a|x=b)$$
or the ***conditional PDF***
  $$f_{y|x}(a,b) = \begin{cases} \Pr(y=a|x=b) & \textrm{if $x$ and $y$ are discrete} \\ \frac{\partial}{\partial a}F_{y|x}(a,b) & \textrm{if $x$ and $y$ are continuous} \\ \end{cases} $$ 

::: example
**Conditional PDFs in roulette**

Let's find the conditional PDF of the payout for a bet on 
14 given the payout for a bet on red. 
  \begin{align}
    \Pr(w_{14}=-1|w_{red}=-1) &= \frac{\Pr(w_{14}=-1 \cap w_{red}=-1)}{\Pr(w_{red}=-1)} \\
      &=\frac{19/37}{19/37} = 1 \\
  \Pr(w_{14}=35|w_{red}=-1) &= \frac{\Pr(w_{14}=35 \cap w_{red}=-1)}{\Pr(w_{red}=-1)} \\
    &=\frac{0}{19/37} = 0 \\
  \Pr(w_{14}=-1|w_{red}=1) &= \frac{\Pr(w_{14}=-1 \cap w_{red}=1)}{\Pr(w_{red}=1)} \\
    &= \frac{17/37}{18/37} \approx 0.944 \\
  \Pr(w_{14}=35|w_{red}=1) &= \frac{\Pr(w_{14}=35 \cap w_{red}=1)}{\Pr(w_{red}=1)} \\
    &= \frac{1/37}{18/37} \approx 0.056
  \end{align}
:::

### Independent random variables

We say that $x$ and $y$ are ***independent*** if every event defined in 
terms of $x$ is independent of every event defined in terms of $y$.
  $$\Pr(x \in A \cap y \in B) = \Pr(x \in A)\Pr(y \in B)$$
As before, independence of $x$ and $y$ implies that the conditional
distribution is the same as the marginal distribution:
  $$\Pr(x \in A| y \in B) = \Pr(x \in A)$$
  $$\Pr(y \in A| x \in B) = \Pr(y \in A)$$
  
Independence also means that the joint and conditional PDF/CDF can be derived
from the marginal PDF/CDF:
  $$f_{x,y}(a,b) = f_x(a)f_y(b)$$
  $$f_{y|x}(a,b) = f_y(a)$$
  $$F_{x,y}(a,b) = F_x(a)F_y(b)$$
  $$F_{y|x}(a,b) = F_y(a)$$
As with independence of events, this will be very handy in simplifying the
analysis.  But remember: independence is an *assumption* that
we can only make when it's reasonable to do so.

::: example
**Independence in roulette**

The winnings from a bet on red ($w_{red}$) and the winnings from 
a bet on 14 ($w_{14}$) in the same game are *not* independent. However
the winnings from a bet on red and a bet on 14 in two different 
games *are* independent since the underlying outcomes are independent.
:::

### Expected values with multiple variables

We earlier showed that if $x$ is a random variable, we can take the
expected value of any function of $x$.  In addition, when $x$ and 
$y$ are random variables with a well-defined joint distribution, 
we can take the expected value of any function of $x$ and $y$.

When $x$ and $y$ are both discrete:
  $$E(g(x,y)) = \sum_{a \in S_x} \sum_{b \in S_y} g(a,b)\Pr( x=a \cap y = b)$$
That is, we add up $g(x,y)$ across all possible values for the pair $(x,y)$
with each value weighted by its probability.

::: fyi
When $x$ and $y$ are both continuous:
  $$E(g(x,y)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(a,b)f_{x,y}(a,b)dadb$$
Again, this looks similar to the formula for the discrete case,
but uses an integral instead of a sum.
:::

As with a single random variable, you can take the expected value inside
a linear function:
  $$E(ax + by + c) = aE(x) + bE(y) + c$$
but not inside most other functions.  For example:
  $$E(xy) \neq E(x)E(y)$$
  $$E(x/y) \neq E(x)/E(y)$$
etc.

::: example
**Multiple bets in roulette**

Suppose we bet \$100 on red and \$10 on 14. Our net payout will be:
  $$w_{total} = 100*w_{red} + 10*w_{14}$$
which has expected value:
  \begin{align}
    E(w_{total}) &= E(100 w_{red} + 10 w_{14}) \\
      &= 100 \, \underbrace{E(w_{red})}_{\approx -0.027} + 10 \, \underbrace{E(w_{14})}_{\approx -0.027} \\
      &\approx -3 
  \end{align}
That is we expect this betting strategy to lose an average of 
about \$3 per game.
:::

### Covariance and correlation

The ***covariance*** of two random variables $x$ and $y$ 
is defined as:
  $$\sigma_{xy} = cov(x,y) = E[(x-E(x))*(y-E(y))]$$
and their ***correlation*** is defined as:
  $$\rho_{xy} = corr(x,y) = \frac{cov(x,y)}{\sqrt{var(x)var(y)}} = \frac{\sigma_{xy}}{\sigma_x\sigma_y}$$
Both the covariance and correlation are measures of how $x$ and $y$ tend to 
move together.  

- When the covariance or correlation is *positive*, then that means that
  $x$ and $y$ tend to move in the *same* direction. When $x$ is high, $y$ is
  typically high, and when $x$ is low, $y$ is typically low.
- When the covariance or correlation is *negative*, then that means that
  $x$ and $y$ tend to move in *opposite* directions. When $x$ is high, $y$ is
  typically low, and when $x$ is low, $y$ is typically high.
- When the covariance or correlation is *zero* or close to zero, 
  then that means that $x$ and $y$ tend *not* to move together.
- Since the standard deviations $\sigma_x$ and $\sigma_y$ are always[^3]
  positive, the covariance and correlation always have the same sign.
- When $x$ and $y$ are independent, their covariance and correlation
  are both exactly zero.  
  - However, it does not go the other way around.  There are many
    pairs of random variables that have a correlation of zero but
    are not independent.
- The covariance always lies somewhere between $-\sigma_x\sigma_y$ and
  $\sigma_x\sigma_y$, and so the correlation always lies between 
  $-1$ and $1$.
  
[^3]: More precisely, one or both of $\sigma_x$ and $\sigma_y$ could be zero.
In that case the covariance will also be zero, and the correlation will be 
undefined (since it is zero divided by zero).

::: example
**Covariance and correlation in roulettte**

The covariance of $w_{red}$ and $w_{14}$ is:
  \begin{align}
    cov(w_{red},w_{14}) &= \begin{aligned}[t]
        & (1-\underbrace{E(w_{red})}_{\approx -0.027})(35-\underbrace{E(w_{14})}_{\approx -0.027})\underbrace{f_{red,14}(1,35)}_{1/37}\\
        &+ (1-\underbrace{E(w_{red})}_{\approx -0.027})(-1-\underbrace{E(w_{14})}_{\approx -0.027})\underbrace{f_{red,14}(1,-1)}_{17/37} \\ 
        &+ (-1-\underbrace{E(w_{red})}_{\approx -0.027})(-1-\underbrace{E(w_{14})}_{\approx -0.027})\underbrace{f_{red,14}(-1,-1)}_{19/37} \\
        \end{aligned} \\
      &\approx 0.999 
    \end{align}
and its correlation is:
  \begin{align}
    corr(w_{red},w_{14}) 
    &= \frac{cov(w_{red},w_{14})}{\sqrt{var(w_{red})*var(w_{14})}} \\
     &\approx \frac{0.999}{\sqrt{1.0*34.1}} \\
     &\approx 0.17
  \end{align}
:::

The covariance is the expected value of a product, and the expected
value is a sum, which implies several convenient properties:

- The covariance can be written:
  $$cov(x,y) = E(xy) - E(x)E(y)$$
- The order in a covariance does not matter:
  $$cov(x,y)=cov(y,x)$$
- Covariances go through sums just like products do:
  $$cov(x,y+z) = cov(x,y) + cov(x,z)$$
- Constant multiples can be taken out of covariances
  $$cov(ax,y) = a cov(x,y)$$
- The variance of a random variable is its covariance with itself:
  $$cov(x,x) = var(x)$$


I will occasionally make use of these properties.

### Many random variables

All of the terminology we have just developed to handle two random 
variables can easily be extended to many more than two random variables.

## Data and the data generating process

Let $D_n$ be our data set. 

- In this chapter, we will assume that $D_n = (x_1,x_2,\ldots,x_n)$ 
  is a data set with one variable and $n$ observations.  We use 
  $x_i$ to refer to an arbitrary observation $i$.
- In most applications, it will be a simple table of numbers with 
  $n$ observations (rows) and $K$ variables (columns). 
- However, it is occasionally something more abstract.  For example,
  the standard problem in the statistical field of machine learning 
  is to develop a system for determining if a particular electronic
  photograph shows a dog or a cat.  In that case, the data is a big 
  folder full of dog and cat photos.

We think of $D_n$ as a set of random variables with unknown joint 
distribution $P_D$.  This joint distribution is called the 
***data generating process*** for $D_n$. The exact DGP
is assumed to be unknown, but we usually have at least some information 
about it.

::: example
**Data from 3 roulette games**

Suppose we have a data set $D_3$ providing the result of $n=3$ independent games of roulette.  Let $b_i$ be the outcome in game $i$, and 
let $x_i$ be the result of a bet on red:
  $$x_i = I(b_i \in RED) = \begin{cases} 1 & b_i \in RED \\ 0 & b_i \notin RED \\ \end{cases}$$
Then $D_n = (x_1,x_2,x_3)$. For example, if red loses the first
two games and wins the third game we have $D_n = (0,0,1)$.
:::

::: example
**The DGP for the roulette data**

The joint distribution of $D_n$ can be derived. Let 
  $$p = \Pr(b \in Red)$$
We showed in a previous chapter that $p \approx 0.486$ if the roulette
wheel is fair.  But rather than assuming it is fair, let's treat $p$
as an unknown parameter.

The PDF of $x_i$ is
  $$f_x(a) = \begin{cases}(1-p) & a = 0 \\ p & a = 1 \\ 0 & \textrm{otherwise} \\ \end{cases} $$
Since the random variables in $D_n$ are independent, their
joint PDF is:
  $$\Pr(D_n) = f_x(x_1)f_x(x_2)f_x(x_3) = p^{x_1+x_2+x_3}(1-p)^{3-x_1-x_2-x_3}$$
Note that even with a small data set of a simple random variable,
the joint PDF is not easy to calculate.  Once we get into larger
data sets and more complex random variables, it can get very 
difficult.  That's OK, we don't usually need to calculate it - we
just need to know that it *could* be calculated.
:::

### Simple random sampling

In order to model the data generating process, we need to model the entire 
joint distribution of $D_n$.  As mentioned earlier, this means we must
model both:

- The marginal probability distribution of each $x_i$
- The relationship between each of the $x_i$'s

Fortunately, we often can simplify this joint distribution quite a bit 
by assuming that $D_n$ 
is ***independent and identically distributed*** (IID) 
or a ***simple random sample*** from a large ***population***.

A simple random sample has two features:

1. **Independent**: Each $x_i$ is independent of the others.
2. **Identically distributed**: Each $x_i$ has the same (unknown)
   marginal distribution.

The reason we call this "independent and identically distributed"
is hopefully obvious, but what does it mean to say we have a 
"random sample" from a "population"? Well, one simple way of 
generating an IID sample is to:

1. Define the population of interest, for example all Canadian residents.
2. Use some purely random mechanism to choose a small subset of cases
  from this population.
   - The subset is called our ***sample***
   - "Purely random" here means some mechanism like a coin flip
     or a computer's random number generator.
   - As a technical matter, the assumption of independence requires
      that we sample *with replacement*. This means we allow
      for the possibility that we sample the same case more than once.
      In practice this doesn't matter as long as the sample is small 
      relative to the population.
3. Collect data from every case in our sample.

It will turn out that a moderately-sized random sample provides 
surprisingly accurate information on the underlying population.

::: example
**Data from roulette**

Our data from 3 roulette games satisfies the criteria
for a simple random sample.

Each observation $x_i$ is an independent random draw from
the $Bernouilli(p)$ distribution where $p = \Pr(b \in Red)$.
:::

### Time series data

Our employment data are an example of ***time series*** data; it is made of
observations of a particular quantity at regularly-spaced points in time.
Most macroeconomic data - GDP, population, inflation, interest rates - are
time series.

Time series have several features that are inconsistent with the random 
sampling assumption:

- They usually have clear *time trends*.  For example, Canada's real 
  GDP has been steadily growing for as long as we have observed it.
  This means that 2010 GDP is drawn from a distribution with a higher
  expected value than the distribution for 1910 GDP.
- They usually have clear recurring cyclical patterns or *seasonality*.  For
  example, unemployment in Canada is usually lower from September 
  through December.
- They usually exhibit what is called *autocorrelation*. Shocks to
  the economy that affect this year's GDP are likely to have 
  a similar (if smaller) effect on next year's GDP.  So the time
  periods close together are more closely related than time periods that 
  are far apart, even after we have accounted for the time trend.

Time series data sometimes require more advanced techniques than we will
learn in this class.

### Other sampling models

Not all useful data sets come from a simple random sample. For example:

- A ***stratified sample*** is collected by dividing the population into
  *strata* (subgroups) based on some observable characteristics, and then randomly
  sampling a predetermined number of cases within each strata. 
  - Most professional surveys are constructed from stratified samples 
    rather than random samples.
  - Stratified sampling is often combined with *oversampling* of 
    some smaller strata that are of particular interest. 
    - The LFS oversamples residents of Prince Edward Island (PEI) because a
      national random sample would not catch enough PEI residents to
      accurately measure PEI's unemployment rate.
    - Government surveys typically oversample disadvantaged groups.
  - Stratified samples can usually be handled as if they were from
    a random sample, with some adjustments.
- A ***cluster sample*** is gathered by dividing the population
  into ***clusters***, randomly selecting some of these clusters, and
  sampling cases within the cluster.  
  - Educational data sets are often gathered this way: we pick a random 
    sample of schools, and then collect data from each student 
    within those schools.
  - Cluster samples can usually be handled as if they were from
    a random sample, with some adjustments.
- A ***census*** gathers data on every case in the population. 
  - For example, we might have data on all 50 US states, or all 
    10 Canadian provinces, or all of the countries of the world. 
  - Data from administrative sources such as tax records or 
    school records often cover the entire population of interest as well. 
  - Censuses are often treated as random samples from some hypothetical
    population of "possible" cases.
- A ***convenience sample*** is gathered by whatever method is convenient.
  - For example, we might gather a survey from people who walk by,
    or we might recruit our friends to participate in the survey.
  - Convenience samples are the worst-case scenario; in many cases they
    simply aren't usable for accurate statistical analysis.
 
Many data sets combine several of these elements.  For example,
Canada's unemployment rate is calculated using data from the 
Labour Force Survey (LFS). The LFS is built from a stratified 
sample of the civilian non-institutionalized working-age population 
of Canada. There is also some clustering: the LFS will typically
interview whole households, and will do some geographic clustering
to save on travel costs. The LFS is gathered monthly, and the
resulting unemployment rate is a time series.

### Sample selection and representativeness

Random samples and their close relatives have the feature that they
are ***representative*** of the population from which they are 
drawn. In a sense that will be made more clear over the next few chapters,
any sufficiently large random sample "looks just like" the population.

Unfortunately, a simple random is quite difficult to collect from
humans. Even if we are able to randomly select cases, we often
run into the following problems:

- ***Nonresponse*** occurs when a sampled individual does not provide
  the information requested by the survey
  - ***Survey-level*** nonresponse occurs when the sampled individual does not
    answer any questions. 
      - This can occur if the sampled individual cannot be found, refuses 
        to answer, or cannot answer (for example, is incapacitated due to
        illness or disability).
      - Recent response rates to telephone surveys have been around 9\%,
        implying over 90\% of those contacted do not respond.
  - ***Item-level*** nonresponse occurs when the sampled individual does 
    not answer a particular question.  
    - This can occur if the respondent refuses to answer, or the 
      question is not applicable or has no valid answer.
    - Item-level nonresponse is particularly common on sensitive questions
      including income.
- ***Censoring*** occurs when a particular quantity of interest cannot be
  observed for a particular case.  Censored outcomes are extremely common
  in economics, for example:
    - In labour market analysis, we cannot observe the market wage for
      individuals who are not currently employed. 
    - In supply/demand analysis, we only observe quantity supplied and
      quantity demanded at the current market price.
      
There are two basic solutions to these problems:

- ***Imputation***: we assume or ***impute*** values for all missing 
  quantities. For example, we might assume that the wage of each 
  non-employed worker is equal to the average wage among employed workers
  with similar characteristics.
- ***Redefinition***: we redefine the population so that our data
  can be correctly interpreted as a random sample from that population.
  For example, instead of having a random sample of *Canadians*, we can
  interpret our data as a random sample of 
  *Canadians who would answer these questions if asked*. 
  
This is not an issue that has a purely technical solution, but requires
careful thought instead.  If we are imputing values, do we believe that
our imputation method is reasonable?  If we are redefining the population,
is that population one we care about?

:::fyi
**Nonresponse bias in recent US elections**

Going into both the 2016 and 2020 US presidential elections, polls
indicated that the Democratic candidate had a substantial lead over
the Republican candidate: Hilary Clinton led Donald Trump by 4-6\% 
nationally in 2016, and Joe Biden led Trump by 8\% nationally in
2020.  The actual vote was much closer, with Clinton winning the
popular vote (but losing the election) by 2\% and Biden winning
the popular vote (and the election) by about 4.5\%.  This pattern
appeared across a wide variety of polls by various organizations
including Republican-affiliated polling companies.

The likely explanation for the disparity is systematic nonresponse: for
one reason or another, Trump voters are less likely to respond to
polls. Since most people do not respond to standard telephone polls
any more (response rates are typically around 9\%), it does not
take much difference in repsonse rates to produce a large difference
in responses.  For example, suppose that voters are equally
split between Trump and Biden, and that 10\% of Biden voters
and 8\% of Trump voters are willing to respond to polls. If you 
call 1000 voters you will get (on average)

- $0.5*1000 = 500$ Trump voters, $0.08*500 =40$ of whom will respond
- $0.5*1000 = 500$ Biden voters, $0.10*500 =50$ of whom will respond

So Biden will have the support of $50/90 = 56\%$ poll respondents
and Trump will have the support of $40/90 = 44\%$.  The polls
show a 12 percentage point gap in support, but actual support is 
even.


Polling organizations are run by statisticians who are well aware
of this problem, and they took various steps between 2016 and 2020
to address it; for example, they now reweight their analysis by 
characteristic like education and income. But the results indicate
that this was not enough; some pollsters have argued that it makes
more sense to just assume the nonresponse bias is 2-3\% and adjust 
the numbers by that amount directly.
:::

