# Data and sampling {#data-and-sampling}

```{r setup6, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      prompt = FALSE,
                      tidy = TRUE,
                      collapse = TRUE)
library("tidyverse")
```

In chapters \@ref(basic-data-cleaning-with-excel) and
\@ref(basic-data-analysis-with-excel) we learned some techniques 
for cleaning data and calculating statistics in Excel.  In chapters 
\@ref(probability-and-random-events) and \@ref(random-variables), 
we developed the basic theory of probability, random events, 
and random variables.  

Our next step is to bring these two strands of concepts together.  Over 
the next two chapters we will develop a framework for talking about data
and the statistics calculated from that data as a random process that can be
described using the theory of probability and random variables.

::: goals
***Chapter goals***

In this chapter we will:

- Interpret the CDF and PDF of a continuous random variable.
- Work with common continuous probability distributions including the 
  uniform and normal.
- Calculate and interpret joint, marginal and conditional distributions
  of two random variables.
- Calculate and interpret the covariance and correlation coefficient of 
  two discrete random variables.
:::

## Continuous random variables {#continuous-random-variables}

So far we have considered random variables with a discrete support. However,
many random variables of interest have a continuous support: they can
take on *any* real value within some range. 

For example, Canada produced 31.251 million metric tons of wheat in 2019. If we
think of that number as a random variable, it's clear that this number
could have been 31.252 million metric tons if circumstances were different. It
also could have been any number between those numbers, for example 31.2511
million or 31.2517 million.

A ***continuous random variable*** has the property that the the probability
of any specific value is zero:
  $$\Pr(x=a) = 0$$
Now this creates something of a paradox: by the rules of probability
the probability that $x$ takes on *some* value is $\Pr(x \in \mathbb{R}) = 1$
but the probability that $x$ takes on any *specific* value is zero.
How can this work?

### The PDF and CDF {#continuous-pdf-and-cdf}

I'll explain how it works with an example.  

::: example
**The standard uniform distribution**

Consider a random variable $x$ that has the ***standard uniform*** 
distribution. What that means is that:

1. The support of $x$ is the range $[0,1]$.
2. All values in this range are equally likely.

The CDF of the standard uniform distribution is:
  $$F_x(a) = \Pr(x \leq a) = \begin{cases} 0 & a < 0 \\ a & a \in [0,1] \\1 & a > 1 \\ \end{cases}$$
Figure \@ref(fig:UniformCDF) below shows the CDF of the standard uniform distribution. 
```{r UniformCDF, fig.cap = "*CDF for the standard uniform distribution*"}
UniformDist <- tibble(a=seq(from=-2,to=2,length.out=100),
                      Fa=punif(seq(from=-2,to=2,length.out=100)),
                      fa=dunif(seq(from=-2,to=2,length.out=100)))
ggplot(data=UniformDist,mapping=aes(x=a,y=Fa)) +
  geom_line(col = "blue") +
  geom_text(x=1,y=0.6,col="blue",label="F_x(a)") +
  xlab("a") +
  ylab("F(a)") +
  labs(title = "Cumulative distribution function (CDF)", 
       subtitle = "Standard uniform", 
       caption = "", 
       tag = "")
```
As we have seen, the CDF of a discrete random variable rises in 
a "stair-step" manner. In contrast, the standard uniform CDF rises 
smoothly with no jumps.  All continuous random variables have a CDF with
this property.

Let $a_1$ and $a_2$ be any two numbers between 0 and 1, and let $a_1 < a_2$.
Then the probability of $x$ being between $a_1$ and $a_2$ is:
  $$\Pr(a_1 < x \leq a_2) = F_x(a_2) - F_x(a_1) = a_2 - a_1$$
As $a_2$ gets closer and closer to $a_1$ this number gets closer and closer
to zero, so the probability of $x$ being *exactly* $a_1$ is zero.
:::

The PDF of a continuous random variable is defined as just the
derivative of its CDF:
  $$f_x(a) = \frac{dF_x(a)}{da}$$

::: example
**The PDF of the standard uniform distribution**

The PDF of a standard uniform random variable is:
  $$f_x(a) = \begin{cases} 0 & a < 0 \\ 1 & a \in [0,1] \\ 0 & a > 1 \\ \end{cases}$$
which looks like this:
```{r UniformPDF, fig.cap = "*PDF for the standard uniform distribution*"}
ggplot(data=UniformDist,mapping=aes(x=a,y=fa)) +
  geom_step(col = "blue") +
  geom_text(x=1.25,y=0.8,col="blue",label="f_x(a)") +
  xlab("a") +
  ylab("f(a)") +
  labs(title = "Probability density function (PDF)", 
       subtitle = "Standard uniform", 
       caption = "", 
       tag = "")
```
:::

Now, in order to work with continuous random variables we would need to
use integral calculus. Integral calculus is taught in MATH 158, which 
is not a prerequisite for the course, So:

  - Most of my examples will be for discrete case.
  - I will briefly show you the math for the continuous case, 
    but I will not expect you to do it.
  - Most of the results I give you will apply for both cases.

::: fyi
**Integral calculus for continuous random variables**

I have defined the PDF for a continuous random variable based on its CDF,
but we can also go the other way and calculate the CDF from the PDF.
The formula for that calculation is:
  $$F_x(a) = \int_{-\infty}^a f_x(v)dv$$
More generally the probability of $x$ being between any two numbers is:

  $$\Pr(a \leq x \leq b) = F_x(b) - F_x(a) = \int_a^b f_x(v)dv$$

Unless you have taken MATH 152 or MATH 158, you may have 
no idea what this is or how to solve it.  That's OK!  All you need
to know for this course is that it *can* be solved.
:::

### Properties of continuous random variables

::: fyi
**The expected value for a continuous random variable**

When $x$ is continuous, its expected value is defined as:
  $$E(x) = \int_{-\infty}^{\infty} af_x(a)da$$
Notice that this looks just like the definition for the discrete case, but
with the sum replaced by an integral sign. 

There is even a general definition that covers both discrete
and continuous variables, as well as any mix between them:
  $$E(x) - \int_{-\infty}^{\infty} a dF_x(a)$$
Again, I do not expect you to understand, remember, or use 
either of these definitions, only to know that they exist.
:::




### The uniform distribution {#uniform-and-standard-uniform}

The ***uniform*** probability distribution is usually written
$$x \sim U(L,H)$$
where $L < H$. It is a continuous probability distribution with 
support $S_x = [L,H]$ and PDF:
$$f_x(a) = \begin{cases}\frac{1}{H-L} & a \in S_x \\ 0 & \textrm{otherwise} \\ \end{cases}$$
The uniform distribution puts equal probability on all values between $L$ 
and $H$. We have already seen the ***standard uniform*** distribution,
which is just the $U(0,1)$ distribution.

::: fyi
Uniform distributions are commonly used by computers because:

- It is easy for a computer to generate a random number from the standard 
  uniform distribution.
- You can generate a random variable with any probability distribution 
  you like by following these steps:
    1. Generate a random variable $q \sim U(0,1)$.
    2. Calculate $x = F^{-1}(q)$ where $F^{-1}$ is the inverse CDF of 
      the distribution you want.

Every video game you have ever played is constantly generating
$U(0,1)$ random numbers and using them to determine the behavior of 
non-player characters, the location of resources, etc.  Without that
element of randomness, these games would be way too predictable to be 
much fun.
:::

The mean and variance of the $U(L,H)$ distribution are:
  $$E(x) = \frac{L+H}{2}$$
  $$var(x) = \frac{(H-L)^2}{12}$$
As with all continuous random variables, these calculations would
require integration which is beyond the scope of this course.

### The normal distribution {#normal-and-standard-normal}

The ***normal distribution*** is typically written as:
$$ x \sim N(\mu,\sigma^2)$$ 
It is a continuous distribution with support $S_x = \mathbb{R}$
and PDF:
$$f_x(a) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(a-\mu)^2}{2\sigma}}$$
The normal distribution is also called the ***Gaussian*** distribution.

The normal distribution looks strange, but it turns out to be a very 
important one in statistics for two reasons:

  1. Any linear function of a normally distributed random variable 
    is also normally distributed.  That is, suppose that 
    $x \sim N(\mu,\sigma^2)$ and let $y = a + bx$ where $a$ and 
    $b$ are any constants.  Then $y \sim N(a+b\mu,b^2\sigma^2)$.
  2. A very important result called the Central Limit Theorem tells
      us that many random variables have a distribution that is
      well-approximated by the normal distribution. We will discuss 
      this in much more detail later.

The mean and variance of a $N(\mu,\sigma^2)$ random variable are:
$$E(x) = \mu$$
$$var(x) = \sigma^2$$

The $N(0,1)$ distribution is also called the ***standard normal distribution***
The standard normal distribution is so useful that we have special
symbol for its PDF:
  $$\phi(a) = \frac{1}{\sqrt{2\pi}} e^{-\frac{a^2}{2}}$$
and its CDF:
  $$\Phi(a) = \int_{-\infty}^a \phi(b)db$$
The standard normal CDF $\Phi(.)$ does not have a closed form
solution, but is easy to calculate on a computer and is available
as a built-in function in Excel, R or any other program used to
analyze data.

Why is this useful?  Well remember that linear functions of 
normal random variables are also normal.  This will allow
us to calculate the CDF of any $N(\mu,\sigma^2)$ random
variable using the standard normal CDF.

Consider a random variable $x \sim N(\mu,\sigma^2)$. Define another
random variable $z = \frac{x-\mu}{\sigma}$. Then:
  $$z \sim N\left(\mu*\frac{1}{\sigma}- \frac{\mu}{\sigma},\sigma^2*\left(\frac{1}{\sigma}\right)^2\right)$$
or equivalently $z \sim N(0,1)$.

This implies:
  \begin{align}
    F_x(a) &= \Pr\left(x \leq a\right) \\
      &= \Pr\left( \frac{x-\mu}{\sigma} \leq \frac{a-\mu}{\sigma}\right)\\
      &= \Pr\left( z \leq \frac{a-\mu}{\sigma}\right) \\ 
      &= \Phi\left(\frac{a-\mu}{\sigma}\right)
  \end{align}
Since the standard normal CDF is available as a built-in function in Excel
or R, so we can use this result to calculate the CDF for any normally
distributed random variable.

## Multiple random variables

Almost all interesting data sets have multiple observations and multiple
variables and observations. So before we start talking about data, we 
need to develop some tools and terminology for thinking about multiple
random variables.

To keep things simple, most of the definitions and examples will be stated in
terms of *two* random variables.  The extension to more than two random 
variables is conceptually straightforward but will be skipped.

### Joint distributions

Let $x = x(b)$ and $y = y(b)$ be two random variables defined in 
terms of the same underlying outcome $b$.

Their ***joint probability distribution*** assigns a probability
to every event that can be defined in terms of $x$ and $y$,
for example $\Pr(x = 6 \cap y = 0)$ or $\Pr(x < y)$.

This joint distribution can be fully described by the ***joint CDF***:
$$F_{x,y}(a,b) = \Pr(x \leq a \cap y \leq b)$$
or by the ***joint PDF***:
$$f_{x,y}(a,b) = \begin{cases}
      \Pr(x = a \cap y = b) & \textrm{if $x$ and $y$ are discrete} \\
      \frac{\partial F_{x,y}(a,b)}{\partial a \partial b} & \textrm{if $x$ and $y$ are continuous} \\
      \end{cases}$$

::: example
**The joint PDF in roulette**

In our roulette example, the joint PDF of $w_{red}$ and 
$w_{14}$ can be derived from the original outcome.

If $b=14$, then both red and 14 win:
  \begin{align}
    f_{red,14}(1,35) &= \Pr(w_{red}=1 \cap w_{14} = 35) \\
      &= \Pr(b \in \{14\}) = 1/37
  \end{align}
If $b \in Red$ but $b \neq 14$, then red wins but 14 loses:
  \begin{align}
    f_{red,14}(1,-1) &= \Pr(w_{red} = 1 \cap w_{14} = -1) \\
      &= \Pr\left(b \in \left\{
        \begin{gathered}
          1,3,5,7,9,12,16,18,19,21,\\
          23,25,27,30,32,34,36
        \end{gathered}\right\}\right)  \\
      &= 17/37
  \end{align}
Otherwise both red and 14 lose:
  \begin{align}
    f_{red,14}(-1,-1) &= \Pr(w_{red} = -1 \cap w_{14} = -1) \\
      &= \Pr\left(b \in \left\{
        \begin{gathered}
          0,2,4,6,7,10,11,13,15,17, \\
          20,22,24,26,28,31,33,35
        \end{gathered}\right\}\right)  \\
      &= 19/37
  \end{align}
All other values have probability zero.
:::

The joint distribution tells you two things about these variables

1. The probability distribution of each individual random variable, 
   sometimes called that variable's ***marginal*** distribution.
   - For example, we can derive each variable's CDF from the joint CDF:
      $$F_x(a) = \Pr(x \leq a) = \Pr(x \leq a \cap y \leq \infty) = F_{x,y}(a,\infty)$$
      $$F_y(b) = \Pr(y \leq n) = \Pr(x \leq \infty \cap y \leq b) = F_{x,y}(\infty,b)$$
   - We can also derive each variable's PDF from the joint PDF
2. The *relationship* between the two variables.  
   - We will develop several ways of describing this relationship: conditional
     distribution, covariance, correlation, etc.

Note that while you can always derive the marginal distributions from the joint 
distribution, you cannot go the other way around unless you know everything
about the relationship between the two variables.

::: example
**Three joint distributions with identical marginal distributions**

The scatter plots in Figure \@ref(fig:JointIsNotMarginal) below 
depict simulation results for a pair of random 
variables $(x,y)$, with a different joint distribution in each
graph. In all three graphs, $x$ and $y$ have the same
marginal distribution (standard normal).

The differences between the graphs are in the relationship 
between $x$ and $y$.  

- In the first graph, $x$ and $y$ are unrelated, so the 
  data looks like as a "cloud" of random dots.  
- In the second graph, $x$ and $y$ have something of a 
  negative relationship. High values of $x$ tend to go
  with low values of $y$.
- In the third graph, $x$ and $y$ are positively and closely 
  related. In fact, they are equal.

```{r JointIsNotMarginal, fig.cap = "*x and y have the same marginal distribution in all three graphs, but not the same joint distribution.*"}
simdata <- tibble(x = rnorm(100),
                  y1 = rnorm(100),
                  y2 = (-x+0.5*y1)/sqrt(1.25),
                  y3 = x)
p1 <- ggplot(data = simdata, mapping = aes(x = x)) +
  geom_point(aes(y=y1),col="blue") +
  xlab("x") + 
  ylab("y")
p2 <- ggplot(data = simdata, mapping = aes(x = x)) +
  geom_point(aes(y=y2),col="blue") +
  xlab("x") + 
  ylab("y")
p3 <- ggplot(data = simdata, mapping = aes(x = x)) +
  geom_point(aes(y=y3),col="blue") +
  xlab("x") + 
  ylab("y")
library("cowplot")
plot_grid(p1,p2,p3,ncol=3,nrow=1)
```


:::


### Conditional distributions

The ***conditional distribution*** of a random variable $y$ given another random
variable $x$ assigns values to all probabilities of the form:
  $$\Pr(y \in A| x \in B) = \frac{\Pr(y \in A \cap x \in B)}{\Pr(x \in B)}$$
Since a conditional probability is just the ratio of the joint probability 
to the marginal probability, the conditional distribution can always be 
derived from the joint distribution.

We can describe a conditional distribution with either the ***conditional CDF***:
  $$F_{y|x}(a,b) = \Pr(y \leq a|x=b)$$
or the ***conditional PDF***
  $$f_{y|x}(a,b) = \begin{cases} \Pr(y=a|x=b) & \textrm{if $x$ and $y$ are discrete} \\ \frac{\partial}{\partial a}F_{y|x}(a,b) & \textrm{if $x$ and $y$ are continuous} \\ \end{cases} $$ 

::: example
**Conditional PDFs in roulette**

Let's find the conditional PDF of the payout for a bet on 
14 given the payout for a bet on red. 
  \begin{align}
    \Pr(w_{14}=-1|w_{red}=-1) &= \frac{\Pr(w_{14}=-1 \cap w_{red}=-1)}{\Pr(w_{red}=-1)} \\
      &=\frac{19/37}{19/37} = 1 \\
  \Pr(w_{14}=35|w_{red}=-1) &= \frac{\Pr(w_{14}=35 \cap w_{red}=-1)}{\Pr(w_{red}=-1)} \\
    &=\frac{0}{19/37} = 0 \\
  \Pr(w_{14}=-1|w_{red}=1) &= \frac{\Pr(w_{14}=-1 \cap w_{red}=1)}{\Pr(w_{red}=1)} \\
    &= \frac{17/37}{18/37} \approx 0.944 \\
  \Pr(w_{14}=35|w_{red}=1) &= \frac{\Pr(w_{14}=35 \cap w_{red}=1)}{\Pr(w_{red}=1)} \\
    &= \frac{1/37}{18/37} \approx 0.056
  \end{align}
:::

### Independent random variables

We say that $x$ and $y$ are ***independent*** if every event defined in 
terms of $x$ is independent of every event defined in terms of $y$.
  $$\Pr(x \in A \cap y \in B) = \Pr(x \in A)\Pr(y \in B)$$
As before, independence of $x$ and $y$ implies that the conditional
distribution is the same as the marginal distribution:
  $$\Pr(x \in A| y \in B) = \Pr(x \in A)$$
  $$\Pr(y \in A| x \in B) = \Pr(y \in A)$$
The first graph in Figure \@ref(fig:JointIsNotMarginal) shows 
what independent random variables look like in data: a cloud of 
unrelated points.
  
Independence also means that the joint and conditional PDF/CDF can be derived
from the marginal PDF/CDF:
  $$f_{x,y}(a,b) = f_x(a)f_y(b)$$
  $$f_{y|x}(a,b) = f_y(a)$$
  $$F_{x,y}(a,b) = F_x(a)F_y(b)$$
  $$F_{y|x}(a,b) = F_y(a)$$
As with independence of events, this will be very handy in simplifying the
analysis.  But remember: independence is an *assumption* that
we can only make when it's reasonable to do so.

::: example
**Independence in roulette**

The winnings from a bet on red $(w_{red})$ and the winnings from 
a bet on 14 $(w_{14})$ in the same game are *not* independent. 

However the winnings from a bet on red and a bet on 14 in two different 
games *are* independent since the underlying outcomes are independent.
:::

### Expected values with multiple variables

We earlier showed that if $x$ is a random variable, we can take the
expected value of any function of $x$, and that you can take
the expected value "inside" any linear function of $x$:
  $$E(a + bx) = a + b E(x)$$
but in general $E(g(x)) \neq g(E(x))$.

Similarly, when $x$ and $y$ are random variables with a well-defined 
joint distribution, we can take the expected value of any function of 
$x$ and $y$. 

::: fyi
When $x$ and $y$ are both discrete:
  $$E(g(x,y)) = \sum_{a \in S_x} \sum_{b \in S_y} g(a,b)\Pr( x=a \cap y = b)$$
That is, we add up $g(x,y)$ across all possible values for the pair $(x,y)$
with each value weighted by its probability.

When $x$ and $y$ are both continuous:
  $$E(g(x,y)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(a,b)f_{x,y}(a,b)dadb$$
Again, this looks similar to the formula for the discrete case,
but uses an integral instead of a sum.

You do not need to remember or use either of these formulas
:::

As with a single random variable, you can take the expected value inside
a *linear* function of multiple random variables:
  $$E(ax + by + c) = aE(x) + bE(y) + c$$
but in general $E(g(x,y)) \neq g(E(x),E(y))$. For example:
  $$E(xy) \neq E(x)E(y)$$
  $$E(x/y) \neq E(x)/E(y)$$
etc.

::: example
**Multiple bets in roulette**

Suppose we bet \$100 on red and \$10 on 14. Our net payout will be:
  $$w_{total} = 100*w_{red} + 10*w_{14}$$
which has expected value:
  \begin{align}
    E(w_{total}) &= E(100 w_{red} + 10 w_{14}) \\
      &= 100 \, \underbrace{E(w_{red})}_{\approx -0.027} + 10 \, \underbrace{E(w_{14})}_{\approx -0.027} \\
      &\approx -3 
  \end{align}
That is we expect this betting strategy to lose an average of 
about \$3 per game.
:::

### Covariance and correlation

The ***covariance*** of two random variables $x$ and $y$ 
is defined as:
  $$\sigma_{xy} = cov(x,y) = E[(x-E(x))*(y-E(y))]$$
and their ***correlation*** is defined as:
  $$\rho_{xy} = corr(x,y) = \frac{cov(x,y)}{\sqrt{var(x)var(y)}} = \frac{\sigma_{xy}}{\sigma_x\sigma_y}$$
Both the covariance and correlation are measures of how $x$ and $y$ tend to 
move together.  

::: example
**Covariance and correlation in roulette**

The covariance of $w_{red}$ and $w_{14}$ is:
  \begin{align}
    cov(w_{red},w_{14}) &= \begin{aligned}[t]
        & (1-\underbrace{E(w_{red})}_{\approx -0.027})(35-\underbrace{E(w_{14})}_{\approx -0.027})\underbrace{f_{red,14}(1,35)}_{1/37}\\
        &+ (1-\underbrace{E(w_{red})}_{\approx -0.027})(-1-\underbrace{E(w_{14})}_{\approx -0.027})\underbrace{f_{red,14}(1,-1)}_{17/37} \\ 
        &+ (-1-\underbrace{E(w_{red})}_{\approx -0.027})(-1-\underbrace{E(w_{14})}_{\approx -0.027})\underbrace{f_{red,14}(-1,-1)}_{19/37} \\
        \end{aligned} \\
      &\approx 0.999 
    \end{align}
and its correlation is:
  \begin{align}
    corr(w_{red},w_{14}) 
    &= \frac{cov(w_{red},w_{14})}{\sqrt{var(w_{red})*var(w_{14})}} \\
     &\approx \frac{0.999}{\sqrt{1.0*34.1}} \\
     &\approx 0.17
  \end{align}
:::

The *sign* of the covariance or correlation tells us the direction
of the relationship between $x$ and $y$.
- If *positive*: $x$ and $y$ tend to move in the *same* direction. 
- If *negative*, $x$ and $y$ tend to move in *opposite* directions.
- If *zero*: $x$ and $y$ tend *not* to move together at all.
the covariance and correlation always have the same sign since 
standard deviations are always[^601] positive, 

[^601]: More precisely, either or both of $\sigma_x$ and $\sigma_y$ could be zero.
In that case the covariance will also be zero, and the correlation will be 
undefined (zero divided by zero).

When random variables are independent, their covariance and correlation are both
exactly zero. However, it does not go the other way around.  

::: example
**Uncorrelated does not imply independent**

Figure \@ref(fig:UncorrelatedButNotIndependent) below shows a scatter plot
from a simulation of two random variables that are clearly related (and therefore
not independent) but whose covariance and correlation are exactly zero.

Intuitively, covariance and correlation are a measure of the *linear* 
relationship between two variables.  When variables have a nonlinear 
relationship as in Figure \@ref(fig:UncorrelatedButNotIndependent), the
covariance or correlation may miss it.

```{r UncorrelatedButNotIndependent, fig.cap = "*x and y are uncorrelated, but clearly related.*"}
simdata <- tibble(x = rnorm(100),
                  y = x^2)
ggplot(data = simdata, mapping = aes(x = x, y =y)) +
  geom_point(col="blue") +
  xlab("x") + 
  ylab("y")
```
:::

The *magnitude* of the covariance or correlation tells us about
the strength of the relationship between the two variables:

- The correlation is a scale-free measure of the strength of the 
  relationship:
  - It always lies between -1 and 1.
  - It is unchanged by any rescaling or change in units.  That is,
    for any positive constants $a$ and $b$:
    $$corr(ax,by) = corr(x,y)$$
  - When $corr(x,y) \in \{-1,1\}$, then that means $y$ is an 
    exact linear function of $x$.  That is, we can write it:
    $$y = a + bx$$
    and
    $$corr(x,y) = \begin{cases} 1 & \textrm{if $b > 0$} \\ -1 & \textrm{if $b < 0$} \\ \end{cases}$$
- In contrast, the covariance depends on the scale of the variables, and
  always lies between $-\sigma_x\sigma_y$ and $\sigma_x\sigma_y$.

Earlier I said that the variance was a square, and has some implied 
properties similar to those of a square.  In particular:

- The variance can be written
  $$var(x) = E(x^2) - E(x)^2$$
- We can take constants out of a variance:
  $$var(ax) = a^2 var(x)$$
since $(ax)^2 = a^2x^2$.

Similarly, the covariance is the expected value of a product, and has some
implied properties similar to those of a product.  

- The covariance can be written:
  $$cov(x,y) = E(xy) - E(x)E(y)$$
- The order in a covariance does not matter:
  $$cov(x,y)=cov(y,x)$$
  since $xy = yx$.
- Covariances go through sums:
  $$cov(x,y+z) = cov(x,y) + cov(x,z)$$
  since $x*(y+z) = xy + xz$
- Constant multiples can be taken out of covariances
  $$cov(ax,by) = ab \, cov(x,y)$$
  since $ax*by = ab*xy$.
- The variance of a random variable is its covariance with itself:
  $$cov(x,x) = var(x)$$
  since $x*x = x^2$
- The variance of a sum is:
  $$var(x + y) = var(x) + var(y) + 2 \, cov(x,y)$$
  since $(x+y)^2 = x^2 + y^2 + 2xy$.

I do not expect you to remember all of these formulas, but be 
prepared to see me use them

