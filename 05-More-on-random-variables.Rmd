# More on random variables {#more-on-random-variables}

```{r setup6, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      prompt = FALSE,
                      tidy = TRUE,
                      collapse = TRUE)
library("tidyverse")
```

Chapter \@ref(random-variables) developed the basic analytical tools
for a single discrete random variable.  This chapter will extend those
tools to continuous random variables, and to multiple random variables.

::: goals
***Chapter goals***

In this chapter we will:

- Interpret the CDF and PDF of a continuous random variable.
- Work with common continuous probability distributions including the 
  uniform and normal.
- Calculate and interpret joint, marginal and conditional distributions
  of two random variables.
- Calculate and interpret the covariance and correlation coefficient of 
  two discrete random variables.
:::

## Continuous random variables {#continuous-random-variables}

So far we have considered random variables with a discrete support. However,
many random variables of interest have a continuous support.

For example, consider the Canadian labour force participation rate.  It is 
defined as:
  $$(\textrm{LFP rate}) = \frac{(\textrm{labour force})}{(\textrm{population})} \times 100\%$$ 
so it can be any (rational) number between 0\% and 100\%:
  $$S_{LFP rate} = [0\%,100\%]$$
This introduces a complication: there is an infinite number of values 
in this support. There is even an infinite number of possible values
between two much closer numbers like 63\% and 63.0001\%.

Because a continuous random variable has an infinite support, it has a seemingly
inconsistent pair of properties:

- The probability that $x$ is *any specific value* in the support is:
  $$\Pr(x = a) = 0$$
  for all $a \in S_x$.
- The probability that $x$ is *somewhere* in the support is:
  $$\Pr(x \in S_x) = 1$$

This feature applies to ranges as well.  For example, the Canadian labour force 
participation rate has a high probability of being between 60\% and 70\%, but 
zero probability of being exactly 65\%.

The math for working with continuous random variables is a little different
from the math for working with discrete random variables, and is harder 
because it requires calculus.  In many cases, it requires integral calculus
(MATH 152 or MATH 158) which is not a prerequisite to this course, and which
I do not expect you to know.

But deep down, there are no really important differences 
between continuous and discrete random variables. The intuition for why is 
straightforward: you can make a continuous random variable into a discrete 
random variable by just rounding it.  For example, suppose you round the 
labour force participation rate to the nearest percentage
point.  Then it becomes a discrete random variable, with support:
  $$S_x = \{0\%, 1\%, \ldots 99\%, 100\% \}$$
The same point applies if you round to the nearest 1/100th of a percentage point,
or the nearest 1/1,000,000th of a percentage point. 

Since discrete and random variables are more alike than first appears,
most of the results and intuition we have already developed for discrete 
random variables can also be applied to continuous random variables. So:

- Most of my examples will be for discrete case.
- I will briefly show you the math for the continuous case, 
  but I will not expect you to do it.
- Most of the results I give you will apply for both cases.

Any time you see an integral here, you can ignore it.

### The continuous CDF {#continuous-pdf-and-cdf}

The CDF of a continuous random variable $x$ is defined exactly the same
way as for the discrete case:
  $$F_x(a) = \Pr(x \leq a)$$
The only difference is how it looks.

If you recall, the CDF of a discrete random variable takes on a stair-step form: 
increasing in discrete jumps at every point in the discrete support, and flat 
everywhere else. 

In contrast, the CDF of a continuous random variable increases 
continuously.  It can have flat parts, but never jumps.

::: example
**The standard uniform distribution**

Consider a random variable $x$ that has the ***standard uniform*** 
distribution. What that means is that:

1. The support of $x$ is the range $[0,1]$.
2. All values in this range are equally likely.

The CDF of the standard uniform distribution is:
  $$F_x(a) = \Pr(x \leq a) = \begin{cases} 0 & a < 0 \\ a & a \in [0,1] \\1 & a > 1 \\ \end{cases}$$
Figure \@ref(fig:StdUniformCDF) below shows the CDF of the standard uniform distribution. 
```{r StdUniformCDF, fig.cap = "*CDF for the standard uniform distribution*"}
UniformDist <- tibble(a=seq(from=-2,to=2,length.out=100),
                      Fa=punif(seq(from=-2,to=2,length.out=100)),
                      fa=dunif(seq(from=-2,to=2,length.out=100)))
ggplot(data=UniformDist,mapping=aes(x=a,y=Fa)) +
  geom_line(col = "blue") +
  geom_text(x=1,y=0.6,col="blue",label="F_x(a)") +
  xlab("a") +
  ylab("F(a)") +
  labs(title = "Cumulative distribution function (CDF)", 
       subtitle = "Standard uniform", 
       caption = "", 
       tag = "")
```
As you can see, the CDF is smoothly increasing betweeen zero and one, and
flat everywhere else.
:::

The CDF of a continuous random variable obeys all of the properties
described in section \@ref(the-cdf):

  $$F_x(a) \leq F_x(b) \qquad \textrm{ if $a \leq b$} $$
  $$0 \leq F_x(a) \leq 1$$
  $$\lim_{a \rightarrow -\infty} F_x(a) = \Pr(x \leq -\infty) = 0$$
  $$\lim_{a \rightarrow \infty} F_x(a) = \Pr(x \leq \infty) = 1$$
  \begin{align}
    F(b) - F(a) &= \Pr(a < x \leq b) 
  \end{align}
In addition, the result on intervals applies to both strict 
and weak inequalities:
  \begin{align}
    F(b) - F(a) &= \Pr(a < x \leq b) \\
      &= \Pr(a < x < b) \\
      &= \Pr(a \leq x \leq b) \\
      &= \Pr(a \leq x < b) 
  \end{align}
since a continuous random variable has probability zero 
of taking on any specific value.

### The continuous PDF {#continuous-pdf}

The PDF $f_x(a)$ for a discrete random variable is defined as the size of the
"jump" in the CDF at $a$, or (equivalently) the probability $\Pr(x=a)$ of 
observing that particular value.  But the CDF of a continuous random variable
has no jumps, and the probability of observing any particular value is
always zero. So this particular function is useless in describing the probability
distribution of a continuous random variable.

Instead, we define the PDF of a continuous random variable $x$ as the slope
or derivative of the CDF:
  $$f_x(a) = \frac{d F_x(a)}{da}$$
In other words, instead of the *amount* the CDF increases (jumps) at $a$,
it is the *rate* at which it increases.

::: example
**The PDF of the standard uniform distribution**

The PDF of a standard uniform random variable is:
  $$f_x(a) = \begin{cases} 0 & a < 0 \\ 1 & a \in [0,1] \\ 0 & a > 1 \\ \end{cases}$$
which looks like this:
```{r StdUniformPDF, fig.cap = "*PDF for the standard uniform distribution*"}
ggplot(data=UniformDist,mapping=aes(x=a,y=fa)) +
  geom_step(col = "blue") +
  geom_text(x=1.25,y=0.8,col="blue",label="f_x(a)") +
  xlab("a") +
  ylab("f(a)") +
  labs(title = "Probability density function (PDF)", 
       subtitle = "Standard uniform", 
       caption = "", 
       tag = "")
```
The PDF of a continuous random variable is a good way to visualize its
probability distribution.  For example, the uniform PDF shows the key 
feature of this distribution: in some loose sense all values in the
support are "equally likely", much like in the 
[discrete uniform distribution](#discrete-uniform)
described earlier. In fact, if you round a uniform random variable, you get
a discrete uniform random variable.
:::

I have defined the PDF in terms of the CDF, but it is also possible to
derive the CDF from the PDF.  This requires integral calculus, so I will
give the definition below but not expect you to use it.

::: fyi
**Deriving the CDF from the PDF of a continuous random variable**

The formula for deriving the CDF of a continuous random variable
from its PDF is:
  $$F_x(a) = \int_{-\infty}^a f_x(v)dv$$
More generally the probability of $x$ being between any two numbers is:

  $$\Pr(a < x \leq b) = F_x(b) - F_x(a) = \int_a^b f_x(v)dv$$
Unless you have taken MATH 152 or MATH 158, you may have 
no idea what this is or how to solve it.  That's OK!  All you need
to know for this course is that it *can* be solved.
:::

### Expected value {#continuous-expected-values}

The definition for the expected value of a continuous random variable 
also uses integral calculus and is given below for background. More 
importantly, the expected value has the same [interpretation]({#the-expected-value}) 
as described earlier for discrete random variables, and it has all of 
the [properties]({#properties-of-the-expected-value}) described earlier 
as well.

Since the variance and standard deviation are both defined as
expected values, they also have the same 
[interpretation and properties]({#variance-and-standard-deviation})
for a continuous random variable as they do for a discrete random variable.

::: fyi
**The expected value for a continuous random variable**

When $x$ is continuous, its expected value is defined as:
  $$E(x) = \int_{-\infty}^{\infty} af_x(a)da$$
Notice that this looks just like the definition for the discrete case, but
with the sum replaced by an integral sign. If you know much about 
integral calculus, you may require that an integral is a sum (or more precisely,
the limit of a sum). This is why the same properties we earlier
found for the expected value of a discrete random variable also 
applies to continuous random variables.

There is even a general definition that covers both discrete
and continuous variables, as well as any mix between them:
  $$E(x) - \int_{-\infty}^{\infty} a dF_x(a)$$
This expression uses notation that is not typically taught in a first
course in integral calculus, so even if you have taken MATH 152 or MATH 158
you may not know how to interpret it.  Again, I am only showing you so
that you know the formula exists, I am not asking you to remember or
use it.
:::

### Quantiles {#continuous-quantiles}

Quantiles and percentiles have the same 
[definition and interpretation]({#quantiles-and-percentiles})
whether the random variable is continuous or discrete.

While most calculations are harder for continuous random variables, calculating
a quantile is actually much easier.  Recall that the $a$ quantile of a random 
variable $x$ is defined as:
 $$q_{\alpha}(x) = \min\{a: \Pr(x \leq a) \geq \alpha\} = \min\{a: F_x(a) \geq \alpha\}$$
When $x$ is continuous, the CDF is continuous and increasing, so the quantile
function is just the inverse of the CDF:
   $$q_{\alpha}(x) = F_x^{-1}(\alpha)$$
Since the median of a random variable is just its 0.5 quantile, the median
of a continuous random variable is also just the inverse CDF:
  $$$M(x) = F_x^{-1}(0.5)$$
The range and percentiles can be derived similarly.

## The uniform distribution {#uniform-and-standard-uniform}

The ***uniform*** probability distribution is usually written
  $$x \sim Uniform(L,H)$$
or
  $$x \sim U(L,H)$$
where $L < H$. 

### The uniform PDF

The uniform distribution is a continuous probability distribution 
with support:
  $$S_x = [L,H]$$
and PDF:
  $$f_x(a) = \begin{cases}\frac{1}{H-L} & a \in S_x \\ 0 & \textrm{otherwise} \\ \end{cases}$$

For example, if $x \sim Uniform(2,5)$ its support is the range of all values from 2 to 5, and its PDF looks like this:
```{r UniformPDF, fig.cap = "*PDF for the Uniform(2,5) distribution*"}
UniformDist <- tibble(a=seq(from=-6,to=6,length.out=100),
                      Fa=punif(seq(from=-6,to=6,length.out=100),min=2,max=5),
                      fa=dunif(seq(from=-6,to=6,length.out=100),min=2,max=5))
ggplot(data=UniformDist,mapping=aes(x=a,y=fa)) +
  geom_step(col = "blue") +
  geom_text(x=1.25,y=0.8,col="blue",label="f_x(a)") +
  xlab("a") +
  ylab("f(a)") +
  labs(title = "Probability density function (PDF)", 
       subtitle = "Uniform(2,5)", 
       caption = "", 
       tag = "")
```
The uniform distribution puts equal probability on all values between $L$ 
and $H$. We have already seen the ***standard uniform*** distribution,
which is just the $U(0,1)$ distribution.

### The uniform CDF

The CDF of the uniform distribution is
  $$f_x(a) = \begin{cases}
    0 & a \leq L \\ 
    \frac{a-L}{H-L} & L < a < H \\ 
    1 & a \geq H \\ 
    \end{cases}$$
For example, if $x \sim Uniform(2,5)$ the CDF looks like this: 
```{r UniformCDF, fig.cap = "*CDF for the Uniform(2,5) distribution*"}
UniformDist <- tibble(a=seq(from=-6,to=6,length.out=100),
                      Fa=punif(seq(from=-6,to=6,length.out=100),min=2,max=5),
                      fa=dunif(seq(from=-6,to=6,length.out=100),min=2,max=5))
ggplot(data=UniformDist,mapping=aes(x=a,y=Fa)) +
  geom_line(col = "blue") +
  geom_text(x=3,y=0.6,col="blue",label="F_x(a)") +
  xlab("a") +
  ylab("F(a)") +
  labs(title = "Cumulative distribution function (CDF)", 
       subtitle = "Uniform(2,5)", 
       caption = "", 
       tag = "")
```
### Median, mean and variance {#uniform-median-mean-and-variance}

The median of a uniform random variable $x \sim Uniform(L,H)$ 
is just the midpoint of its support:
  $$Med(x) = F_x^{-1}(0.5) = \frac{L+H}{2}$$
Its mean can be found using integral calculus, and is also the
midpoint:
  $$E(x) = \frac{L+H}{2}$$
Its variance also requires integral calculus:
  $$var(x) = \frac{(H-L)^2}{12}$$
and its standard deviation is just the square root of the variance:
  $$sd(x) = \sqrt{\frac{(H-L)^2}{12}}$$
as always.

::: fyi
**Uniform distributions in video games**

Uniform distributions are important in many computer applications
including video games.  

- It is easy for a computer to generate a random number from the 
  $Uniform(0,1)$ distribution.
- You can generate a random variable with any probability distribution 
  you like by following these steps:
    1. Generate a random variable $q \sim Uniform(0,1)$.
    2. Calculate $x = F^{-1}(q)$ where $F^{-1}$ is the inverse CDF of 
      the distribution you want.

Every video game you have ever played is constantly generating
$Uniform(0,1)$ random numbers and using them to determine the behavior of 
non-player characters, the location of weapons and other resources, 
etc.  Without that element of randomness, these games would be far 
too predictable to be much fun.
:::

## The normal distribution {#normal-and-standard-normal}

The ***normal distribution*** is typically written as:
  $$ x \sim N(\mu,\sigma^2)$$ 
The normal distribution is also called the ***Gaussian*** distribution,
and the $N(0,1)$ distribution is called the ***standard normal*** 
distribution.

### The normal PDF

The $N(\mu,\sigma^2)$ distribution is a continuous distribution with support 
$S_x = \mathbb{R}$ and PDF:
$$f_x(a) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(a-\mu)^2}{2\sigma}}$$

For example, the PDF for the $N(0,1)$ distribution looks like
this:
```{r StNormalPDF, fig.cap = "*PDF for the N(0,1) distribution*"}
NormalDist <- tibble(a=seq(from=-6,to=6,length.out=100),
                      Fa=pnorm(seq(from=-6,to=6,length.out=100)),
                      fa=dnorm(seq(from=-6,to=6,length.out=100)))
ggplot(data=NormalDist,mapping=aes(x=a,y=fa)) +
  geom_line(col = "blue") +
  geom_text(x=1.25,y=0.8,col="blue",label="f_x(a)") +
  xlab("a") +
  ylab("f(a)") +
  labs(title = "Probability density function (PDF)", 
       subtitle = "N(0,1)", 
       caption = "", 
       tag = "")
```
As the figure shows, the $N(0,1)$ distribution is symmetric around 
$\mu=0$ and bell-shaped, meaning that it is usually close to zero 
but can occasionally be quite far.  

For other values of $\mu$, the $N(\mu,\sigma^2)$ distribution
is also symmetric around $\mu$ and bell-shaped, with the "spread"
of the distribution depending on the value of $\sigma^2$.
```{r NormalPDF, fig.cap = "*PDF for several normal distributions*"}
NormalDist <- tibble(a=seq(from=-6,to=6,length.out=100),
                      Fa=pnorm(seq(from=-6,to=6,length.out=100)),
                      fa1=dnorm(seq(from=-6,to=6,length.out=100),mean=1),
                      fa2=dnorm(seq(from=-6,to=6,length.out=100),sd=2),
                      fa=dnorm(seq(from=-6,to=6,length.out=100)))
ggplot(data=NormalDist,mapping=aes(x=a,y=fa)) +
  geom_line(col = "blue") +
  geom_line(aes(y=fa1),col = "red") +
  geom_line(aes(y=fa2),col = "purple") +
  geom_text(x=-2,y=0.3,col="blue",label="N(0,1)") +
  geom_text(x=3,y=0.3,col="red",label="N(1,1)") +
  geom_text(x=-4.5,y=0.05,col="purple",label="N(0,2)") +
  xlab("a") +
  ylab("f(a)") +
  labs(title = "Probability density function (PDF)", 
       subtitle = "", 
       caption = "", 
       tag = "")
```


### The normal CDF

The CDF of the normal distribution can be derived by integrating the PDF. There
is no simple closed-form expression for this CDF, but it is easy to calculate 
with a computer.  
```{r NormalCDF, fig.cap = "*PDF for several normal distributions*"}
NormalDist <- tibble(a=seq(from=-6,to=6,length.out=100),
                      Fa=pnorm(seq(from=-6,to=6,length.out=100)),
                      Fa1=pnorm(seq(from=-6,to=6,length.out=100),mean=1),
                      Fa2=pnorm(seq(from=-6,to=6,length.out=100),sd=2),
                      fa=dnorm(seq(from=-6,to=6,length.out=100)))
ggplot(data=NormalDist,mapping=aes(x=a,y=Fa)) +
  geom_line(col = "blue") +
  geom_line(aes(y=Fa1),col = "red") +
  geom_line(aes(y=Fa2),col = "purple") +
  geom_text(x=0,y=0.8,col="blue",label="N(0,1)") +
  geom_text(x=1,y=0.3,col="red",label="N(1,1)") +
  geom_text(x=-3.2,y=0.13,col="purple",label="N(0,2)") +
  xlab("a") +
  ylab("f(a)") +
  labs(title = "Cumulative distribution function (CDF)", 
       subtitle = "", 
       caption = "", 
       tag = "")
```
The Excel function `NORM.DIST()` can be used to calculate the 
PDF or CDF of the normal distribution, and the Excel function `NORM.INV()` can 
be used to calculate its quantile (inverse CDF) function.

### Median, mean and variance

The mean and variance of a $N(\mu,\sigma^2)$ random variable can be found
by integration:
  $$E(x) = \mu$$
  $$var(x) = \sigma^2$$
and the standard deviation is just the square root of the variance:
  $$sd(x) = \sigma$$
The median of a $N(\mu,\sigma^2)$ random variable is also $\mu$.

### Linear functions of normals

The formula for the normal PDF may look strange, but it implies a 
very important characteristic: any linear function of a normal random
variable is also normal.  That is, if 
  $$x \sim N(\mu,\sigma^2)$$
Then for any constants $a$ and $b$:
  $$ax + b \sim N(a\mu + b, b^2\sigma^2)$$
Just to be clear, we already showed that $E(ax +b) = aE(x) + b$ and 
$var(ax+b) = a^2 var(x)$ for any random variable $x$.  What is new here 
is that $ax +b$ is normally distributed if $x$ is.

### The standard normal distribution

The standard normal distribution is so useful that we have special
symbol for its PDF:
  $$\phi(a) = \frac{1}{\sqrt{2\pi}} e^{-\frac{a^2}{2}}$$
and its CDF:
  $$\Phi(a) = \int_{-\infty}^a \phi(b)db$$
All statistical programs, including Excel and R, provide a function 
that calculates the standard normal CDF.

Our result in the previous section about linear functions of a normal
can be used to restate the distribution of any normally distributed 
random variable in terms of a standard normal.  That is,
suppose that 
  $$x \sim N(\mu, \sigma^2)$$
Then if we define
  $$z = \frac{x - \mu}{\sigma}$$
our result implies that
  $$z \sim N\left(\frac{\mu-\mu}{\sigma}, 
                  \left(\frac{1}{\sigma}\right)^2 \sigma^2\right)$$
or equivalently:
  $$z \sim N(0,1)$$
We can use this result to derive the CDF of $x$:
  \begin{align}
    F_x(a) &= \Pr\left(x \leq a\right) \\
      &= \Pr\left( \frac{x-\mu}{\sigma} \leq \frac{a-\mu}{\sigma}\right)\\
      &= \Pr\left( z \leq \frac{a-\mu}{\sigma}\right) \\ 
      &= \Phi\left(\frac{a-\mu}{\sigma}\right)
  \end{align}
Since the standard normal CDF is available as a built-in function in Excel
or R, so we can use this result to calculate the CDF for any normally
distributed random variable.

::: fyi
A very important result called the Central Limit Theorem tells
us that many "real world" random variables have a probability distribution 
that is well-approximated by the normal distribution. 

We will discuss the central limit theorem in much more detail 
later.
:::

## Multiple random variables

Almost all interesting data sets have multiple observations and multiple
variables and observations. So before we start talking about data, we 
need to develop some tools and terminology for thinking about multiple
random variables.

To keep things simple, most of the definitions and examples will be stated in
terms of *two* random variables.  The extension to more than two random 
variables is conceptually straightforward but will be skipped.

### Joint distributions

Let $x = x(b)$ and $y = y(b)$ be two random variables defined in 
terms of the same underlying outcome $b$.

Their ***joint probability distribution*** assigns a probability
to every event that can be defined in terms of $x$ and $y$,
for example $\Pr(x = 6 \cap y = 0)$ or $\Pr(x < y)$.

This joint distribution can be fully described by the ***joint CDF***:
$$F_{x,y}(a,b) = \Pr(x \leq a \cap y \leq b)$$
or by the ***joint PDF***:
$$f_{x,y}(a,b) = \begin{cases}
      \Pr(x = a \cap y = b) & \textrm{if $x$ and $y$ are discrete} \\
      \frac{\partial F_{x,y}(a,b)}{\partial a \partial b} & \textrm{if $x$ and $y$ are continuous} \\
      \end{cases}$$

::: example
**The joint PDF in roulette**

In our roulette example, the joint PDF of $w_{red}$ and 
$w_{14}$ can be derived from the original outcome.

If $b=14$, then both red and 14 win:
  \begin{align}
    f_{red,14}(1,35) &= \Pr(w_{red}=1 \cap w_{14} = 35) \\
      &= \Pr(b \in \{14\}) = 1/37
  \end{align}
If $b \in Red$ but $b \neq 14$, then red wins but 14 loses:
  \begin{align}
    f_{red,14}(1,-1) &= \Pr(w_{red} = 1 \cap w_{14} = -1) \\
      &= \Pr\left(b \in \left\{
        \begin{gathered}
          1,3,5,7,9,12,16,18,19,21,\\
          23,25,27,30,32,34,36
        \end{gathered}\right\}\right)  \\
      &= 17/37
  \end{align}
Otherwise both red and 14 lose:
  \begin{align}
    f_{red,14}(-1,-1) &= \Pr(w_{red} = -1 \cap w_{14} = -1) \\
      &= \Pr\left(b \in \left\{
        \begin{gathered}
          0,2,4,6,7,10,11,13,15,17, \\
          20,22,24,26,28,31,33,35
        \end{gathered}\right\}\right)  \\
      &= 19/37
  \end{align}
All other values have probability zero.
:::

The joint distribution tells you two things about these variables

1. The probability distribution of each individual random variable, 
   sometimes called that variable's ***marginal*** distribution.
   - For example, we can derive each variable's CDF from the joint CDF:
      $$F_x(a) = \Pr(x \leq a) = \Pr(x \leq a \cap y \leq \infty) = F_{x,y}(a,\infty)$$
      $$F_y(b) = \Pr(y \leq n) = \Pr(x \leq \infty \cap y \leq b) = F_{x,y}(\infty,b)$$
   - We can also derive each variable's PDF from the joint PDF
2. The *relationship* between the two variables.  
   - We will develop several ways of describing this relationship: conditional
     distribution, covariance, correlation, etc.

Note that while you can always derive the marginal distributions from the joint 
distribution, you cannot go the other way around unless you know everything
about the relationship between the two variables.

::: example
**Three joint distributions with identical marginal distributions**

The scatter plots in Figure \@ref(fig:JointIsNotMarginal) below 
depict simulation results for a pair of random 
variables $(x,y)$, with a different joint distribution in each
graph. In all three graphs, $x$ and $y$ have the same
marginal distribution (standard normal).

The differences between the graphs are in the relationship 
between $x$ and $y$.  

- In the first graph, $x$ and $y$ are unrelated, so the 
  data looks like as a "cloud" of random dots.  
- In the second graph, $x$ and $y$ have something of a 
  negative relationship. High values of $x$ tend to go
  with low values of $y$.
- In the third graph, $x$ and $y$ are positively and closely 
  related. In fact, they are equal.

```{r JointIsNotMarginal, fig.cap = "*x and y have the same marginal distribution in all three graphs, but not the same joint distribution.*"}
simdata <- tibble(x = rnorm(100),
                  y1 = rnorm(100),
                  y2 = (-x+0.5*y1)/sqrt(1.25),
                  y3 = x)
p1 <- ggplot(data = simdata, mapping = aes(x = x)) +
  geom_point(aes(y=y1),col="blue") +
  xlab("x") + 
  ylab("y")
p2 <- ggplot(data = simdata, mapping = aes(x = x)) +
  geom_point(aes(y=y2),col="blue") +
  xlab("x") + 
  ylab("y")
p3 <- ggplot(data = simdata, mapping = aes(x = x)) +
  geom_point(aes(y=y3),col="blue") +
  xlab("x") + 
  ylab("y")
library("cowplot")
plot_grid(p1,p2,p3,ncol=3,nrow=1)
```


:::


### Conditional distributions

The ***conditional distribution*** of a random variable $y$ given another random
variable $x$ assigns values to all probabilities of the form:
  $$\Pr(y \in A| x \in B) = \frac{\Pr(y \in A \cap x \in B)}{\Pr(x \in B)}$$
Since a conditional probability is just the ratio of the joint probability 
to the marginal probability, the conditional distribution can always be 
derived from the joint distribution.

We can describe a conditional distribution with either the ***conditional CDF***:
  $$F_{y|x}(a,b) = \Pr(y \leq a|x=b)$$
or the ***conditional PDF***
  $$f_{y|x}(a,b) = \begin{cases} \Pr(y=a|x=b) & \textrm{if $x$ and $y$ are discrete} \\ \frac{\partial}{\partial a}F_{y|x}(a,b) & \textrm{if $x$ and $y$ are continuous} \\ \end{cases} $$ 

::: example
**Conditional PDFs in roulette**

Let's find the conditional PDF of the payout for a bet on 
14 given the payout for a bet on red. 
  \begin{align}
    \Pr(w_{14}=-1|w_{red}=-1) &= \frac{\Pr(w_{14}=-1 \cap w_{red}=-1)}{\Pr(w_{red}=-1)} \\
      &=\frac{19/37}{19/37} = 1 \\
  \Pr(w_{14}=35|w_{red}=-1) &= \frac{\Pr(w_{14}=35 \cap w_{red}=-1)}{\Pr(w_{red}=-1)} \\
    &=\frac{0}{19/37} = 0 \\
  \Pr(w_{14}=-1|w_{red}=1) &= \frac{\Pr(w_{14}=-1 \cap w_{red}=1)}{\Pr(w_{red}=1)} \\
    &= \frac{17/37}{18/37} \approx 0.944 \\
  \Pr(w_{14}=35|w_{red}=1) &= \frac{\Pr(w_{14}=35 \cap w_{red}=1)}{\Pr(w_{red}=1)} \\
    &= \frac{1/37}{18/37} \approx 0.056
  \end{align}
:::

### Independence {#independent-random-variables}

We say that $x$ and $y$ are ***independent*** if every event defined in 
terms of $x$ is independent of every event defined in terms of $y$.
  $$\Pr(x \in A \cap y \in B) = \Pr(x \in A)\Pr(y \in B)$$
As before, independence of $x$ and $y$ implies that the conditional
distribution is the same as the marginal distribution:
  $$\Pr(x \in A| y \in B) = \Pr(x \in A)$$
  $$\Pr(y \in A| x \in B) = \Pr(y \in A)$$
The first graph in Figure \@ref(fig:JointIsNotMarginal) shows 
what independent random variables look like in data: a cloud of 
unrelated points.
  
Independence also means that the joint and conditional PDF/CDF can be derived
from the marginal PDF/CDF:
  $$f_{x,y}(a,b) = f_x(a)f_y(b)$$
  $$f_{y|x}(a,b) = f_y(a)$$
  $$F_{x,y}(a,b) = F_x(a)F_y(b)$$
  $$F_{y|x}(a,b) = F_y(a)$$
As with independence of events, this will be very handy in simplifying the
analysis.  But remember: independence is an *assumption* that
we can only make when it's reasonable to do so.

::: example
**Independence in roulette**

The winnings from a bet on red $(w_{red})$ and the winnings from 
a bet on 14 $(w_{14})$ in the same game are *not* independent. 

However the winnings from a bet on red and a bet on 14 in two different 
games *are* independent since the underlying outcomes are independent.
:::

### Expected values {#multiple-expected-values}

We earlier showed that if $x$ is a random variable, we can take the
expected value of any function of $x$, and that you can take
the expected value "inside" any linear function of $x$:
  $$E(a + bx) = a + b E(x)$$
but in general $E(g(x)) \neq g(E(x))$.

Similarly, when $x$ and $y$ are random variables with a well-defined 
joint distribution, we can take the expected value of any function of 
$x$ and $y$. 

::: fyi
When $x$ and $y$ are both discrete:
  $$E(g(x,y)) = \sum_{a \in S_x} \sum_{b \in S_y} g(a,b)\Pr( x=a \cap y = b)$$
That is, we add up $g(x,y)$ across all possible values for the pair $(x,y)$
with each value weighted by its probability.

When $x$ and $y$ are both continuous:
  $$E(g(x,y)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(a,b)f_{x,y}(a,b)dadb$$
Again, this looks similar to the formula for the discrete case,
but uses an integral instead of a sum.

You do not need to remember or use either of these formulas
:::

As with a single random variable, you can take the expected value inside
a *linear* function of multiple random variables:
  $$E(ax + by + c) = aE(x) + bE(y) + c$$
but in general $E(g(x,y)) \neq g(E(x),E(y))$. For example:
  $$E(xy) \neq E(x)E(y)$$
  $$E(x/y) \neq E(x)/E(y)$$
etc.

::: example
**Multiple bets in roulette**

Suppose we bet \$100 on red and \$10 on 14. Our net payout will be:
  $$w_{total} = 100*w_{red} + 10*w_{14}$$
which has expected value:
  \begin{align}
    E(w_{total}) &= E(100 w_{red} + 10 w_{14}) \\
      &= 100 \, \underbrace{E(w_{red})}_{\approx -0.027} + 10 \, \underbrace{E(w_{14})}_{\approx -0.027} \\
      &\approx -3 
  \end{align}
That is we expect this betting strategy to lose an average of 
about \$3 per game.
:::

### Covariance

The ***covariance*** of two random variables $x$ and $y$ 
is defined as:
  $$\sigma_{xy} = cov(x,y) = E[(x-E(x))*(y-E(y))]$$
The covariance is a measure of how $x$ and $y$ tend to move together.  

::: example
**Covariance in roulette**

The covariance of $w_{red}$ and $w_{14}$ is:
  \begin{align}
    cov(w_{red},w_{14}) &= \begin{aligned}[t]
        & (1-\underbrace{E(w_{red})}_{\approx -0.027})(35-\underbrace{E(w_{14})}_{\approx -0.027})\underbrace{f_{red,14}(1,35)}_{1/37}\\
        &+ (1-\underbrace{E(w_{red})}_{\approx -0.027})(-1-\underbrace{E(w_{14})}_{\approx -0.027})\underbrace{f_{red,14}(1,-1)}_{17/37} \\ 
        &+ (-1-\underbrace{E(w_{red})}_{\approx -0.027})(-1-\underbrace{E(w_{14})}_{\approx -0.027})\underbrace{f_{red,14}(-1,-1)}_{19/37} \\
        \end{aligned} \\
      &\approx 0.999 
    \end{align}
:::

The *sign* of the covariance tells us the direction
of the relationship between $x$ and $y$.
- If *positive*: $x$ and $y$ tend to move in the *same* direction. 
- If *negative*, $x$ and $y$ tend to move in *opposite* directions.
- If *zero*: $x$ and $y$ tend *not* to move together at all.

When random variables are independent, their covariance is
exactly zero. However, it does not go the other way around.  

::: example
**Zero covariance does not imply independent**

Figure \@ref(fig:UncorrelatedButNotIndependent) below shows a scatter plot
from a simulation of two random variables that are clearly related (and therefore
not independent) but whose covariance is exactly zero.

Intuitively, covariance is a measure of the *linear* 
relationship between two variables.  When variables have a nonlinear 
relationship as in Figure \@ref(fig:UncorrelatedButNotIndependent), the
covariance may miss it.

```{r UncorrelatedButNotIndependent, fig.cap = "*x and y are uncorrelated, but clearly related.*"}
simdata <- tibble(x = rnorm(100),
                  y = x^2)
ggplot(data = simdata, mapping = aes(x = x, y =y)) +
  geom_point(col="blue") +
  xlab("x") + 
  ylab("y")
```
:::

The *magnitude* of the covariance or correlation tells us about
the strength of the relationship between the two variables:

- In contrast, the covariance depends on the scale of the variables, and
  always lies between $-\sigma_x\sigma_y$ and $\sigma_x\sigma_y$.

Earlier I said that the variance was a square, and has some implied 
properties similar to those of a square.  In particular:

- The variance can be written
  $$var(x) = E(x^2) - E(x)^2$$
- We can take constants out of a variance:
  $$var(ax) = a^2 var(x)$$
  since $(ax)^2 = a^2x^2$.

Similarly, the covariance is the expected value of a product, and has some
implied properties similar to those of a product.  

- The covariance can be written:
  $$cov(x,y) = E(xy) - E(x)E(y)$$
- The order in a covariance does not matter:
  $$cov(x,y)=cov(y,x)$$
  since $xy = yx$.
- Covariances go through sums:
  $$cov(x,y+z) = cov(x,y) + cov(x,z)$$
  since $x*(y+z) = xy + xz$
- Constant multiples can be taken out of covariances
  $$cov(ax,by) = ab \, cov(x,y)$$
  since $ax*by = ab*xy$.
- The variance of a random variable is its covariance with itself:
  $$cov(x,x) = var(x)$$
  since $x*x = x^2$
- The variance of a sum is:
  $$var(x + y) = var(x) + var(y) + 2 \, cov(x,y)$$
  since $(x+y)^2 = x^2 + y^2 + 2xy$.

I do not expect you to remember all of these formulas, but be 
prepared to see me use them

### Correlation

The ***correlation coefficient*** of two random variables $x$ and $y$ is 
defined as:
  $$\rho_{xy} = corr(x,y) = \frac{cov(x,y)}{\sqrt{var(x)var(y)}} = \frac{\sigma_{xy}}{\sigma_x\sigma_y}$$
Like the covariance, the correlation describes the strength of a (linear) 
relationship between $x$ and $y$.  But it is resecaled in a way that makes
it more convenient for some purposes.

::: example
**Correlation in roulette**

The correlation of $w_{red}$ and $w_{14}$ is:
  \begin{align}
    corr(w_{red},w_{14}) 
    &= \frac{cov(w_{red},w_{14})}{\sqrt{var(w_{red})*var(w_{14})}} \\
     &\approx \frac{0.999}{\sqrt{1.0*34.1}} \\
     &\approx 0.17
  \end{align}
:::


The covariance and correlation always have the same sign since 
standard deviations are always[^601] positive. The key
difference between them is that correlation is scale-invariant. That
is:

- It always lies between -1 and 1.
- It is unchanged by any rescaling or change in units.  That is,
  for any positive constants $a$ and $b$:
  $$corr(ax,by) = corr(x,y)$$
- When $corr(x,y) \in \{-1,1\}$, then that means $y$ is an 
  exact linear function of $x$.  That is, we can write it:
  $$y = a + bx$$
  and
  $$corr(x,y) = \begin{cases} 1 & \textrm{if $b > 0$} \\ -1 & \textrm{if $b < 0$} \\ \end{cases}$$

[^601]: More precisely, either or both of $\sigma_x$ and $\sigma_y$ could be zero.
In that case the covariance will also be zero, and the correlation will be 
undefined (zero divided by zero).

## Chapter review {#review-more-on-random-variables}

To be added

### Practice problems {#problems-more-on-random-variables}

To be added.
